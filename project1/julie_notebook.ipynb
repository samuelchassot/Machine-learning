{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_stdrzed, mean, std = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_gammas(function, y, tx, initial_w, max_iters, gammas, k_fold, seed):\n",
    "    \"\"\"Do cross-validation to find the best gamma to use on a given function\"\"\"\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    mse_tr = []\n",
    "    mse_te = []\n",
    "    \n",
    "    weights = initial_w\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        tr_tmp = []\n",
    "        te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            # divide the data into training set and testing set depending on k\n",
    "            tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "            test_tx = tx[k_indices[k]]\n",
    "            test_y = y[k_indices[k]]\n",
    "            train_tx = tx[tr_indice]\n",
    "            train_y = y[tr_indice]\n",
    "            \n",
    "            #Train the set and computes the losses\n",
    "            weights, loss_tr = function(train_y, train_tx, initial_w, max_iters, gamma)\n",
    "            loss_te = compute_loss(mse, test_y, test_tx, weights)\n",
    "            \n",
    "            tr_tmp.append(loss_tr)\n",
    "            te_tmp.append(loss_te)\n",
    "        mse_tr.append(np.mean(tr_tmp))\n",
    "        mse_te.append(np.mean(te_tmp))\n",
    "        \n",
    "    return mse_tr, mse_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000 #100 for least_squares\n",
    "k_fold = 5\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#least_squares_GD cross-validation\n",
    "initial_w = np.array([0.4 for i in range(tX_stdrzed.shape[1])]) \n",
    "# ATTENTION !!!! 1. return nan pour loss avec least_squares_gd !!!\n",
    "gammas = np.array([0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])\n",
    "mse_tr_least_squares_GD, mse_te_least_squares_GD = \\\n",
    "    cross_validation_gammas(least_squares_GD, y, tX_stdrzed, initial_w, max_iters, gammas, k_fold, seed)\n",
    "\n",
    "\n",
    "print(mse_tr_least_squares_GD)\n",
    "print(mse_te_least_squares_GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights for best gamma for least_squares_GD\n",
    "\n",
    "gamma = gammas[np.argmin(mse_te_least_squares_GD)]\n",
    "weights, loss = least_squares_GD(y, tX_stdrzed, initial_w, max_iters, gamma)\n",
    "\n",
    "print(gamma)\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic_regression cross-validation\n",
    "\n",
    "\n",
    "#IL FAUT CHANGER LES LABELS POUR Y DE -1/1 Ã  0/1!!!!!!!!\n",
    "y_logistic = []\n",
    "for elem in y:\n",
    "    if elem == -1:\n",
    "        y_logistic.append(0)\n",
    "    else:\n",
    "        y_logistic.append(1)\n",
    "        \n",
    "y_logistic = np.asarray(y_logistic)\n",
    "        \n",
    "initial_w = np.array([0.5 for i in range(tX_stdrzed.shape[1])])\n",
    "gammas = np.array([0.0000001, 0.000001, 0.00001])\n",
    "mse_tr_logistic_regression, mse_te_logistic_regression = \\\n",
    "cross_validation_gammas(logistic_regression, y_logistic, tX_stdrzed, initial_w, max_iters, gammas, k_fold, seed)\n",
    "\n",
    "print(mse_tr_logistic_regression)\n",
    "print(mse_te_logistic_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = gammas[np.argmin(mse_te_logistic_regression)]\n",
    "weights, loss = logistic_regression(y_logistic, tX_stdrzed, initial_w, max_iters, gamma)\n",
    "\n",
    "print(gamma)\n",
    "print(weights)\n",
    "print(loss)\n",
    "\n",
    "y_pred = predict_labels(weights, tX_test_stdrzd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_stdrzd = (tX_test-mean)/std #USE THE MEAN AND STD OF TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(weights, tX_test_stdrzd)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#least_squares_GD:\n",
    "\n",
    "#max_iters = 100 \n",
    "#k_fold = 5\n",
    "#seed = 42\n",
    "#initial_w = np.array([0.4 for i in range(tX_stdrzed.shape[1])]) \n",
    "#gamma : 0.08\n",
    "#weigths: [ 0.21327586 -0.1146719  -0.07368819  0.04893754  0.02225969  0.12098264\n",
    "#  0.01944926  0.11727695  0.05210771 -0.26103589  0.1158705   0.12397883\n",
    "#  0.02394943  0.11838262  0.12186015  0.12181576 -0.00271883  0.121864\n",
    "#  0.12193441  0.04248975  0.12208034 -0.37029438  0.11649633 -0.0311746\n",
    "#  0.06456901  0.06459276 -0.08128091  0.024305    0.02420671 -0.13281751]\n",
    "#loss : 0.4187127661494721\n",
    "\n",
    "# On AICrowd :  \n",
    "# Categorical Accuracy : 0.672 \n",
    "# F1-Score : 0.339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic_regression:\n",
    "\n",
    "#max_iters = 1000 \n",
    "#k_fold = 5\n",
    "#seed = 42\n",
    "#initial_w = np.array([0.5 for i in range(tX_stdrzed.shape[1])])\n",
    "#gamma : 1e-07\n",
    "#[  6.70968093 -35.37478552  -3.81622818  -0.19319657   2.63897026\n",
    "#   2.21089965   2.48368267   0.90021442  -1.06382924   2.97263477\n",
    "#  -0.65038781   1.45159447   2.70114369  18.05969885   0.24352419\n",
    "#   0.17975174  -4.73092368   0.23675558   0.34633217   0.58217815\n",
    "#   0.32184767  -3.89345914   0.13572754   3.97257752   4.31809735\n",
    "#   4.34578467   0.38030864   2.58058968   2.55730877  -9.82233767]\n",
    "#loss : 2847310.0023890096 ???\n",
    "\n",
    "\n",
    "# On AICrowd :  \n",
    "# Categorical Accuracy : 0.696 \n",
    "# F1-Score : 0.267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least squares with cleaned colums\n",
    "\n",
    "tX_stdrzed_cleaned, mean, std = standardize(remove_wrong_columns(tX))\n",
    "\n",
    "max_iters = 200\n",
    "k_fold = 5\n",
    "seed = 42\n",
    "initial_w = np.array([0.1 for i in range(tX_stdrzed_cleaned.shape[1])]) \n",
    "gammas = np.array([0.01, 0.05, 0.09, 0.1, 0.2, 0.3])\n",
    "\n",
    "mse_tr_least_squares_GD_cleaned, mse_te_least_squares_GD_cleaned = \\\n",
    "    cross_validation_gammas(least_squares_GD, y, tX_stdrzed_cleaned, initial_w, max_iters, gammas, k_fold, seed)\n",
    "\n",
    "print(mse_tr_least_squares_GD_cleaned)\n",
    "print(mse_te_least_squares_GD_cleaned)\n",
    "\n",
    "gamma = gammas[np.argmin(mse_te_least_squares_GD_cleaned)]\n",
    "weights, loss = least_squares_GD(y, tX_stdrzed_cleaned, initial_w, max_iters, gamma)\n",
    "\n",
    "print(gamma)\n",
    "print(weights)\n",
    "print(loss)\n",
    "\n",
    "tX_test_stdrzd_cleaned = (remove_wrong_columns(tX_test)-mean)/std #USE THE MEAN AND STD OF TRAINING DATA\n",
    "\n",
    "y_pred = predict_labels(weights, tX_test_stdrzd_cleaned)\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "OUTPUT_PATH = 'data/output.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "\n",
    "#max_iters = 200 : It outputs 71.7% of accuracy\n",
    "#with max_iters = 1000 : the loss was converging to 0.374 which was quite close to the loss with max_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:67: RuntimeWarning: divide by zero encountered in log\n",
      "  + (1 - y).T @ np.log(1 - sigm_tx_w))\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:134: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if np.abs(loss - previous_loss) < threshold:\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:67: RuntimeWarning: invalid value encountered in matmul\n",
      "  + (1 - y).T @ np.log(1 - sigm_tx_w))\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "  return - np.sum(y.T @ np.log(sigm_tx_w) \\\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:66: RuntimeWarning: invalid value encountered in matmul\n",
      "  return - np.sum(y.T @ np.log(sigm_tx_w) \\\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:60: RuntimeWarning: overflow encountered in exp\n",
      "  t_exp = np.exp(t)\n",
      "/Users/julie/Documents/Repos/Machine-learning/project1/implementations.py:61: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return t_exp /(1 + t_exp)\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression with cleaned colums\n",
    "\n",
    "tX_stdrzed_cleaned, mean, std = standardize(remove_wrong_columns(tX))\n",
    "\n",
    "y_logistic = []\n",
    "for elem in y:\n",
    "    if elem == -1:\n",
    "        y_logistic.append(0)\n",
    "    else:\n",
    "        y_logistic.append(1)\n",
    "y_logistic = np.asarray(y_logistic)\n",
    "\n",
    "max_iters = 1000\n",
    "k_fold = 5\n",
    "seed = 42\n",
    "        \n",
    "initial_w = np.array([0.5 for i in range(tX_stdrzed_cleaned.shape[1])])\n",
    "gammas = np.array([0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1])\n",
    "\n",
    "mse_tr_logistic_cleaned, mse_te_logistic_cleaned = \\\n",
    "    cross_validation_gammas(logistic_regression, y_logistic, tX_stdrzed_cleaned, initial_w, max_iters, gammas, k_fold, seed)\n",
    "\n",
    "print(mse_tr_logistic_cleaned)\n",
    "print(mse_te_logistic_cleaned)\n",
    "\n",
    "gamma = gammas[np.argmin(mse_te_logistic_cleaned)]\n",
    "weights, loss = logistic_regression(y_logistic, tX_stdrzed_cleaned, initial_w, max_iters, gamma)\n",
    "\n",
    "print(gamma)\n",
    "print(weights)\n",
    "print(loss)\n",
    "\n",
    "tX_test_stdrzd_cleaned = (remove_wrong_columns(tX_test)-mean)/std #USE THE MEAN AND STD OF TRAINING DATA\n",
    "\n",
    "y_pred = predict_labels(weights, tX_test_stdrzd_cleaned)\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
