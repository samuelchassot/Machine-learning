{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 4]\n",
      " [2 1]\n",
      " [0 3]]\n",
      "(array([3, 2, 1, 4]), array([-1,  1,  1, -1]), array([6, 5]), array([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "y1 = np.array([1,1,-1,-1, 1, 1])\n",
    "x1 = np.array([1,2,3,4,5,6])\n",
    "k = 3\n",
    "k_indices = build_k_indices(y1, k, 23)\n",
    "print(k_indices)\n",
    "print(get_tr_and_te(y1, x1, k_indices, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    cost = 0.0\n",
    "    n = len(y)\n",
    "    for i in range(0, len(y)):\n",
    "        cost += (np.log(1 + np.exp(tx[i].T@w)) - y[i]*tx[i].T@w)/n\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T@(sigmoid(tx@w) - y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    \n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    w = w - gamma*gradient\n",
    "    \n",
    "    return loss, w\n",
    "\n",
    "def my_logistic_reg(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "            weights_k = np.zeros((k_fold))\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                #w_k, tr_loss_k = my_logistic_reg(tr_y_k, tr_tx_k, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            weight = np.mean(weights_k)\n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammas =  [1.e-05 1.e-06 1.e-07 1.e-08 1.e-09]\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "max_iters = 2500\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(5,10)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss = 0.6931471805599424\n",
      "At iteration 1000, loss = 1.121579901871062\n",
      "At iteration 2000, loss = 2.2034012683175748\n",
      "At iteration 0, loss = 1.119733951912261\n",
      "At iteration 1000, loss = 2.170528846816451\n",
      "At iteration 2000, loss = 3.5900982935283174\n",
      "At iteration 0, loss = 2.164408863360957\n",
      "At iteration 1000, loss = 3.59228204399471\n",
      "At iteration 2000, loss = 1.1180637840816787\n",
      "2.303868309007598\n",
      "188329.42507864253\n",
      "At iteration 0, loss = 3.899812979794199\n",
      "At iteration 1000, loss = 1.363073988541969\n",
      "At iteration 2000, loss = 2.282972025802477\n",
      "At iteration 0, loss = 1.3590409028983332\n",
      "At iteration 1000, loss = 2.2545735519186496\n",
      "At iteration 2000, loss = 4.00493039952471\n",
      "At iteration 0, loss = 2.2484722559278416\n",
      "At iteration 1000, loss = 4.01149020909986\n",
      "At iteration 2000, loss = 1.3535656551665716\n",
      "2.55122919993676\n",
      "206391.57946863945\n",
      "At iteration 0, loss = 4.88609634014767\n",
      "At iteration 1000, loss = 1.4834045476067004\n",
      "At iteration 2000, loss = 3.3010983438979813\n",
      "At iteration 0, loss = 1.479517938774186\n",
      "At iteration 1000, loss = 3.2521128340715655\n",
      "At iteration 2000, loss = 5.922390378886659\n",
      "At iteration 0, loss = 3.2483283812959423\n",
      "At iteration 1000, loss = 5.91883978142671\n",
      "At iteration 2000, loss = 1.4816290851254896\n",
      "3.5683851917233333\n",
      "262455.05417477485\n",
      "At iteration 0, loss = 4.972456463462031\n",
      "At iteration 1000, loss = 0.5717561537302223\n",
      "At iteration 2000, loss = 0.566333028488318\n",
      "At iteration 0, loss = 0.5645547958855358\n",
      "At iteration 1000, loss = 0.5628589510812299\n",
      "At iteration 2000, loss = 0.5620199430803419\n",
      "At iteration 0, loss = 0.562020288821137\n",
      "At iteration 1000, loss = 0.561692262384142\n",
      "At iteration 2000, loss = 0.561527040464483\n",
      "0.562691748061942\n",
      "46111.65263856768\n",
      "At iteration 0, loss = 0.6701786966017743\n",
      "At iteration 1000, loss = 0.5893929768387274\n",
      "At iteration 2000, loss = 0.5883917558984019\n",
      "At iteration 0, loss = 0.5878707905368664\n",
      "At iteration 0, loss = 0.5882175584881981\n",
      "0.5881522552569051\n",
      "48150.96442321127\n",
      "At iteration 0, loss = 0.68100858795091\n",
      "At iteration 0, loss = 0.6132734777478956\n",
      "At iteration 0, loss = 0.6136466745282458\n",
      "0.6135508285782032\n",
      "50171.34025715573\n",
      "At iteration 0, loss = 0.6023484997541558\n",
      "At iteration 1000, loss = 0.5870737778846279\n",
      "At iteration 2000, loss = 0.5826697448280601\n",
      "At iteration 0, loss = 0.580681740278657\n",
      "At iteration 1000, loss = 0.5786172465306745\n",
      "At iteration 2000, loss = 0.5770399540614314\n",
      "At iteration 0, loss = 0.5767277324637821\n",
      "At iteration 1000, loss = 0.5754908875601338\n",
      "At iteration 2000, loss = 0.5744059052110996\n",
      "0.5771813593949648\n",
      "47976.50875492025\n",
      "At iteration 0, loss = 0.5916070464858894\n",
      "At iteration 1000, loss = 0.5896619806963727\n",
      "At iteration 2000, loss = 0.5890013682583704\n",
      "At iteration 0, loss = 0.5883588383297466\n",
      "At iteration 1000, loss = 0.5881434534628396\n",
      "At iteration 2000, loss = 0.5880307334073318\n",
      "At iteration 0, loss = 0.5883404836239747\n",
      "At iteration 1000, loss = 0.5882887697482765\n",
      "At iteration 2000, loss = 0.5882597163810906\n",
      "0.5883590178726784\n",
      "48023.37592976342\n",
      "At iteration 0, loss = 0.6873446067146719\n",
      "At iteration 1000, loss = 0.6145402613436329\n",
      "At iteration 2000, loss = 0.6137495153101704\n",
      "At iteration 0, loss = 0.6132763432283503\n",
      "At iteration 0, loss = 0.6136480003235381\n",
      "0.6135527114984418\n",
      "50167.63744549725\n",
      "At iteration 0, loss = 0.6023151353036225\n",
      "At iteration 1000, loss = 0.5988997471790992\n",
      "At iteration 2000, loss = 0.5963647929624449\n",
      "At iteration 0, loss = 0.5947706733370387\n",
      "At iteration 1000, loss = 0.5929571750010574\n",
      "At iteration 2000, loss = 0.5914766919424499\n",
      "At iteration 0, loss = 0.5912468501591553\n",
      "At iteration 1000, loss = 0.5901050300422308\n",
      "At iteration 2000, loss = 0.589123081726028\n",
      "0.591610322227384\n",
      "49277.18838234839\n",
      "At iteration 0, loss = 0.5924852050745776\n",
      "At iteration 1000, loss = 0.5921405905575443\n",
      "At iteration 2000, loss = 0.5918382998558613\n",
      "At iteration 0, loss = 0.5911026108458103\n",
      "At iteration 1000, loss = 0.5908489080498913\n",
      "At iteration 2000, loss = 0.5906237175884966\n",
      "At iteration 0, loss = 0.5909248806507664\n",
      "At iteration 1000, loss = 0.5907258409717883\n",
      "At iteration 2000, loss = 0.5905490968928069\n",
      "0.5908960886984952\n",
      "48833.843797162095\n",
      "At iteration 0, loss = 0.6382765361747235\n",
      "At iteration 1000, loss = 0.6273848187590712\n",
      "At iteration 2000, loss = 0.6214004759536285\n",
      "At iteration 0, loss = 0.6190004054453213\n",
      "At iteration 1000, loss = 0.616559095044593\n",
      "At iteration 2000, loss = 0.6151859083280703\n",
      "At iteration 0, loss = 0.6150980713950625\n",
      "At iteration 1000, loss = 0.6145038458411961\n",
      "At iteration 2000, loss = 0.6141597551753336\n",
      "0.6160985576487418\n",
      "49693.059736618154\n",
      "At iteration 0, loss = 0.5999015647465225\n",
      "At iteration 1000, loss = 0.5995245689319153\n",
      "At iteration 2000, loss = 0.5991816422504437\n",
      "At iteration 0, loss = 0.5985311505487504\n",
      "At iteration 1000, loss = 0.5982013308971627\n",
      "At iteration 2000, loss = 0.5978837176697392\n",
      "At iteration 0, loss = 0.598106783126862\n",
      "At iteration 1000, loss = 0.5978053758794813\n",
      "At iteration 2000, loss = 0.5975137963978351\n",
      "0.5980392103524864\n",
      "49823.19543210779\n",
      "At iteration 0, loss = 0.599089660864476\n",
      "At iteration 1000, loss = 0.598886250621154\n",
      "At iteration 2000, loss = 0.5986893719218687\n",
      "At iteration 0, loss = 0.5980843093472789\n",
      "At iteration 1000, loss = 0.5978915549651462\n",
      "At iteration 2000, loss = 0.5977052116838921\n",
      "At iteration 0, loss = 0.5979968707544251\n",
      "At iteration 1000, loss = 0.5978194856193416\n",
      "At iteration 2000, loss = 0.5976482511791131\n",
      "0.5979242456666257\n",
      "49664.663319556916\n",
      "At iteration 0, loss = 0.616070813046643\n",
      "At iteration 1000, loss = 0.6159071707736159\n",
      "At iteration 2000, loss = 0.6157678095495602\n",
      "At iteration 0, loss = 0.6151950995729567\n",
      "At iteration 1000, loss = 0.6150772193360361\n",
      "At iteration 2000, loss = 0.6149672663001337\n",
      "At iteration 0, loss = 0.6152863060662328\n",
      "At iteration 1000, loss = 0.6151855444696661\n",
      "At iteration 2000, loss = 0.6150922303438208\n",
      "0.6152221204792555\n",
      "49695.29474786993\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "#w, loss = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change lambdas and gammas seeing this result to reduce computation time\n",
    "before best was \n",
    "gamma =  1e-06  lambda =  100\n",
    "At iteration 6000, loss = 0.5555634390152187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammas =  [1.e-06 1.e-07]\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "max_iters = 3000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000])\n",
    "gammas = np.array([10**(-i) for i in range(6,8)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss = 0.6931471805599424\n",
      "At iteration 1000, loss = 0.5684430043539948\n",
      "At iteration 2000, loss = 0.5583692247724278\n",
      "At iteration 0, loss = 0.5511264311615018\n",
      "At iteration 1000, loss = 0.5460653088954509\n",
      "At iteration 2000, loss = 0.5422474944368155\n",
      "At iteration 0, loss = 0.5394698316506356\n",
      "At iteration 1000, loss = 0.5371208207547129\n",
      "At iteration 2000, loss = 0.5352654732448939\n",
      "0.5414543076955408\n",
      "45091.577688956146\n",
      "At iteration 0, loss = 0.5370512202125811\n",
      "At iteration 1000, loss = 0.5364209571518908\n",
      "At iteration 2000, loss = 0.5359194013519164\n",
      "At iteration 0, loss = 0.5357680009964346\n",
      "At iteration 1000, loss = 0.5354509790411728\n",
      "At iteration 2000, loss = 0.5351932177944126\n",
      "At iteration 0, loss = 0.5350665959556371\n",
      "At iteration 1000, loss = 0.534877907388126\n",
      "At iteration 2000, loss = 0.5347233412289472\n",
      "0.5350280068978794\n",
      "44142.23929225857\n",
      "At iteration 0, loss = 0.5866304758119318\n",
      "At iteration 1000, loss = 0.574730786235232\n",
      "At iteration 2000, loss = 0.5684202081603393\n",
      "At iteration 0, loss = 0.5651287933612793\n",
      "At iteration 1000, loss = 0.5632774313471997\n",
      "At iteration 2000, loss = 0.5622962789955954\n",
      "At iteration 0, loss = 0.561978578972674\n",
      "At iteration 1000, loss = 0.5616957254329297\n",
      "At iteration 2000, loss = 0.5615433122178519\n",
      "0.5627696839663471\n",
      "45206.79040552307\n",
      "At iteration 0, loss = 0.7013432344789441\n",
      "At iteration 1000, loss = 0.5898339010894509\n",
      "At iteration 2000, loss = 0.5883980665868284\n",
      "At iteration 0, loss = 0.5878706328093243\n",
      "At iteration 0, loss = 0.5882176202214217\n",
      "0.5881522672408991\n",
      "48151.174581485444\n",
      "At iteration 0, loss = 0.5781013986914636\n",
      "At iteration 1000, loss = 0.5751928187416517\n",
      "At iteration 2000, loss = 0.5732137295266682\n",
      "At iteration 0, loss = 0.5711065322549079\n",
      "At iteration 1000, loss = 0.5696994079172696\n",
      "At iteration 2000, loss = 0.568408863067738\n",
      "At iteration 0, loss = 0.5675392206767453\n",
      "At iteration 1000, loss = 0.5663784170023537\n",
      "At iteration 2000, loss = 0.5652761056856023\n",
      "0.5676675770989607\n",
      "47299.63229514132\n",
      "At iteration 0, loss = 0.5646107452923562\n",
      "At iteration 1000, loss = 0.5636325570785344\n",
      "At iteration 2000, loss = 0.5626984323269585\n",
      "At iteration 0, loss = 0.5614995418429929\n",
      "At iteration 1000, loss = 0.5606636124136273\n",
      "At iteration 2000, loss = 0.5598624825534152\n",
      "At iteration 0, loss = 0.5593891988653916\n",
      "At iteration 1000, loss = 0.5586335062109851\n",
      "At iteration 2000, loss = 0.5579095886888801\n",
      "0.5593694671501261\n",
      "46561.75119352859\n",
      "At iteration 0, loss = 0.5636731329973643\n",
      "At iteration 1000, loss = 0.5634899568134148\n",
      "At iteration 2000, loss = 0.5633331198260721\n",
      "At iteration 0, loss = 0.5629794580928594\n",
      "At iteration 1000, loss = 0.562852757579079\n",
      "At iteration 2000, loss = 0.5627389637020143\n",
      "At iteration 0, loss = 0.5628979536073923\n",
      "At iteration 1000, loss = 0.5627916902690071\n",
      "At iteration 2000, loss = 0.5626969017670104\n",
      "0.5628114022181784\n",
      "46218.92016923975\n",
      "At iteration 0, loss = 0.6404190075313767\n",
      "At iteration 1000, loss = 0.6209394269409043\n",
      "At iteration 2000, loss = 0.6090932848730942\n",
      "At iteration 0, loss = 0.601320317075298\n",
      "At iteration 1000, loss = 0.596515372404462\n",
      "At iteration 2000, loss = 0.5934391048182723\n",
      "At iteration 0, loss = 0.5917743226788691\n",
      "At iteration 1000, loss = 0.5905090672112355\n",
      "At iteration 2000, loss = 0.5896956210086127\n",
      "0.5940966313989348\n",
      "47525.07009379773\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma =  1e-06  lambda =  10\n",
      "At iteration 0, loss = 0.5328341757248475\n",
      "At iteration 1000, loss = 0.532452677185637\n",
      "At iteration 2000, loss = 0.5321516183711678\n",
      "At iteration 3000, loss = 0.531909762256691\n",
      "At iteration 4000, loss = 0.5317122514246776\n",
      "At iteration 5000, loss = 0.5315485407960628\n",
      "At iteration 6000, loss = 0.5314110377112972\n",
      "At iteration 7000, loss = 0.5312941953740249\n",
      "At iteration 8000, loss = 0.5311939013393535\n",
      "At iteration 9000, loss = 0.5311070605521278\n",
      "At iteration 10000, loss = 0.531031308152806\n",
      "At iteration 11000, loss = 0.5309648097321319\n",
      "At iteration 12000, loss = 0.5309061210691471\n",
      "At iteration 13000, loss = 0.5308540886833469\n",
      "At iteration 14000, loss = 0.5308077786240801\n",
      "At iteration 15000, loss = 0.5307664249543416\n",
      "At iteration 16000, loss = 0.5307293920817708\n",
      "At iteration 17000, loss = 0.5306961469057752\n",
      "At iteration 18000, loss = 0.5306662379823048\n",
      "At iteration 19000, loss = 0.5306392797501206\n",
      "At iteration 20000, loss = 0.53061494044167\n",
      "At iteration 21000, loss = 0.5305929327025244\n",
      "At iteration 22000, loss = 0.5305730062223469\n",
      "At iteration 23000, loss = 0.5305549418757505\n",
      "At iteration 24000, loss = 0.5305385470090548\n",
      "At iteration 25000, loss = 0.5305236516065505\n",
      "At iteration 26000, loss = 0.530510105139514\n",
      "At iteration 27000, loss = 0.5304977739512655\n",
      "At iteration 28000, loss = 0.5304865390677854\n"
     ]
    }
   ],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w, loss = reg_logistic_regression(y, tX_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best with regularized logistic regression\n",
    "0.737\n",
    "\n",
    "gamma =  1e-06  lambda =  10\n",
    "At iteration 28000, loss = 0.5304865390677854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_standardized = (tX_test- tr_mean)/tr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_losses_gamma_10e6 = np.array(list(zip(te_losses[0], lambdas_)))\n",
    "te_losses_gamma_10e7 = np.array(list(zip(te_losses[1], lambdas_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b435a20>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([te_losses_gamma_10e6[i][1] for i in range(len(te_losses_gamma_10e6))], [te_losses_gamma_10e6[i][0] for i in range(len(te_losses_gamma_10e6))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50915777e+04, 1.00000000e+00],\n",
       "       [4.41422393e+04, 1.00000000e+01],\n",
       "       [4.52067904e+04, 1.00000000e+02],\n",
       "       [4.81511746e+04, 1.00000000e+03]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_losses_gamma_10e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try with cleaned data\n",
    "\n",
    "it is also the clean path for my part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y\n",
    "\n",
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "            weights_k = np.zeros((k_fold))\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                #w_k, tr_loss_k = my_logistic_reg(tr_y_k, tr_tx_k, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            weight = np.mean(weights_k)\n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "max_iters = 2000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(6,10)])\n",
    "tX_cleaned_standardized, tr_cleaned_mean, tr_cleaned_std = standardize(remove_wrong_columns(tX))\n",
    "w_initial = np.array([0.0 for i in range(tX_cleaned_standardized.shape[1])])\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss = 0.6931471805599424\n",
      "At iteration 1000, loss = 0.548942295593559\n",
      "At iteration 0, loss = 0.5491499759130487\n",
      "At iteration 1000, loss = 0.5489350828653689\n",
      "At iteration 0, loss = 0.5490875795572286\n",
      "At iteration 1000, loss = 0.5489792864662135\n",
      "0.5486163263423302\n",
      "45714.578255253124\n",
      "At iteration 0, loss = 0.548046939012589\n",
      "At iteration 1000, loss = 0.547958845903764\n",
      "At iteration 0, loss = 0.5489024641115755\n",
      "At iteration 1000, loss = 0.5488316514943153\n",
      "At iteration 0, loss = 0.5490570397033575\n",
      "At iteration 1000, loss = 0.5489736802190576\n",
      "0.5485271957860199\n",
      "45673.126194797944\n",
      "At iteration 0, loss = 0.5522842520962964\n",
      "At iteration 1000, loss = 0.5519722214426114\n",
      "At iteration 0, loss = 0.5528498388125059\n",
      "At iteration 1000, loss = 0.5527934574157167\n",
      "At iteration 0, loss = 0.5529860572849336\n",
      "At iteration 1000, loss = 0.5529378617596414\n",
      "0.552523856772055\n",
      "45709.44035950503\n",
      "At iteration 0, loss = 0.586470882047375\n",
      "At iteration 1000, loss = 0.5726847015294565\n",
      "At iteration 0, loss = 0.5733293221110204\n",
      "At iteration 0, loss = 0.5734161747469444\n",
      "0.5731131164997852\n",
      "46422.75798597116\n",
      "At iteration 0, loss = 0.7163981884292417\n",
      "At iteration 0, loss = 0.6208323791318522\n",
      "At iteration 0, loss = 0.6209718855845345\n",
      "0.6208043033234654\n",
      "49912.38236832877\n",
      "At iteration 0, loss = 0.5988514842607153\n",
      "At iteration 1000, loss = 0.5662940766526451\n",
      "At iteration 0, loss = 0.5575998675632254\n",
      "At iteration 1000, loss = 0.5540684507593765\n",
      "At iteration 0, loss = 0.5524801847618562\n",
      "At iteration 1000, loss = 0.5515711785860301\n",
      "0.5534382344416781\n",
      "46108.557473534165\n",
      "At iteration 0, loss = 0.5502933308593353\n",
      "At iteration 1000, loss = 0.5498912733772783\n",
      "At iteration 0, loss = 0.5505565975141408\n",
      "At iteration 1000, loss = 0.55035141601276\n",
      "At iteration 0, loss = 0.5503027596773553\n",
      "At iteration 1000, loss = 0.5501782646867593\n",
      "0.5499577814253366\n",
      "45800.68531596472\n",
      "At iteration 0, loss = 0.5520904435979516\n",
      "At iteration 1000, loss = 0.5520194929316423\n",
      "At iteration 0, loss = 0.5529454424127462\n",
      "At iteration 1000, loss = 0.5529093536125104\n",
      "At iteration 0, loss = 0.5530416319491483\n",
      "At iteration 1000, loss = 0.5530227769793538\n",
      "0.5526262762851016\n",
      "45770.87164267473\n",
      "At iteration 0, loss = 0.5823039328193402\n",
      "At iteration 1000, loss = 0.5761422347589762\n",
      "At iteration 0, loss = 0.5748540717639732\n",
      "At iteration 1000, loss = 0.5740170805586999\n",
      "At iteration 0, loss = 0.5737900620830948\n",
      "At iteration 1000, loss = 0.57360360379107\n",
      "0.5737476659098028\n",
      "46261.47423238171\n",
      "At iteration 0, loss = 0.7234603501222094\n",
      "At iteration 1000, loss = 0.6212355595064688\n",
      "At iteration 0, loss = 0.6208399813120052\n",
      "At iteration 0, loss = 0.6209729531608479\n",
      "0.6208071767731564\n",
      "49898.15295248461\n",
      "At iteration 0, loss = 0.5987428396355868\n",
      "At iteration 1000, loss = 0.5919672150371871\n",
      "At iteration 0, loss = 0.5874269565079302\n",
      "At iteration 1000, loss = 0.5835227833315593\n",
      "At iteration 0, loss = 0.5803161061260289\n",
      "At iteration 1000, loss = 0.5773580491858171\n",
      "0.5806877764066061\n",
      "48385.139302648466\n",
      "At iteration 0, loss = 0.5742437557588436\n",
      "At iteration 1000, loss = 0.5719224434960226\n",
      "At iteration 0, loss = 0.5703773849275642\n",
      "At iteration 1000, loss = 0.5686066112667075\n",
      "At iteration 0, loss = 0.5671284161870325\n",
      "At iteration 1000, loss = 0.5657109317854583\n",
      "0.5671165825611475\n",
      "47248.66453513765\n",
      "At iteration 0, loss = 0.5647523680375145\n",
      "At iteration 1000, loss = 0.5637064469537186\n",
      "At iteration 0, loss = 0.563400511775348\n",
      "At iteration 1000, loss = 0.5625893847150062\n",
      "At iteration 0, loss = 0.5619342773706697\n",
      "At iteration 1000, loss = 0.5612713780308152\n",
      "0.5617684991299486\n",
      "46700.12388936192\n",
      "At iteration 0, loss = 0.5730341277748235\n",
      "At iteration 1000, loss = 0.5729924463734153\n",
      "At iteration 0, loss = 0.5736486925879463\n",
      "At iteration 1000, loss = 0.5736250149330998\n",
      "At iteration 0, loss = 0.5736673578425359\n",
      "At iteration 1000, loss = 0.5736472704213805\n",
      "0.573398632558146\n",
      "46554.19569469793\n",
      "At iteration 0, loss = 0.7063387740512453\n",
      "At iteration 1000, loss = 0.6705927397643242\n",
      "At iteration 0, loss = 0.6504601533285325\n",
      "At iteration 1000, loss = 0.6382440134892299\n",
      "At iteration 0, loss = 0.6311263053668363\n",
      "At iteration 1000, loss = 0.6269259312551786\n",
      "0.6351631447903241\n",
      "48354.60498762113\n",
      "At iteration 0, loss = 0.5879330948429382\n",
      "At iteration 1000, loss = 0.5872369706196291\n",
      "At iteration 0, loss = 0.5869306902996095\n",
      "At iteration 1000, loss = 0.5863336221390919\n",
      "At iteration 0, loss = 0.5858794042960932\n",
      "At iteration 1000, loss = 0.5853548594208186\n",
      "0.5857406886287081\n",
      "48810.110508010024\n",
      "At iteration 0, loss = 0.5844299161398577\n",
      "At iteration 1000, loss = 0.5839442137476121\n",
      "At iteration 0, loss = 0.5838365556098218\n",
      "At iteration 1000, loss = 0.583396752035438\n",
      "At iteration 0, loss = 0.5830957609377417\n",
      "At iteration 1000, loss = 0.5826905069198588\n",
      "0.5829174182721898\n",
      "48571.66043311\n",
      "At iteration 0, loss = 0.582226115714934\n",
      "At iteration 1000, loss = 0.5818512473033377\n",
      "At iteration 0, loss = 0.5818633984323613\n",
      "At iteration 1000, loss = 0.5815153240085468\n",
      "At iteration 0, loss = 0.5813049552356082\n",
      "At iteration 1000, loss = 0.5809750098742421\n",
      "0.5811054852599781\n",
      "48382.74094212159\n",
      "At iteration 0, loss = 0.584799668995235\n",
      "At iteration 1000, loss = 0.5846024640581979\n",
      "At iteration 0, loss = 0.5848042206567459\n",
      "At iteration 1000, loss = 0.5846204783043519\n",
      "At iteration 0, loss = 0.584569476043229\n",
      "At iteration 1000, loss = 0.5843924159230974\n",
      "0.5843563989780928\n",
      "48243.74619095817\n",
      "At iteration 0, loss = 0.6335599222830669\n",
      "At iteration 1000, loss = 0.6326953286059409\n",
      "At iteration 0, loss = 0.6323089167771866\n",
      "At iteration 1000, loss = 0.6315847111918819\n",
      "At iteration 0, loss = 0.6310325773410641\n",
      "At iteration 1000, loss = 0.6304144540040977\n",
      "0.6308920865431314\n",
      "48370.52061476809\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_cleaned_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma =  1e-06  lambda =  10\n",
      "At iteration 0, loss = 0.5814523041074868\n",
      "At iteration 1000, loss = 0.5492832628327505\n",
      "At iteration 2000, loss = 0.548913053101627\n",
      "At iteration 3000, loss = 0.5487664291932653\n",
      "At iteration 4000, loss = 0.548645170178941\n",
      "At iteration 5000, loss = 0.548531900406382\n",
      "At iteration 6000, loss = 0.5484247160010013\n",
      "At iteration 7000, loss = 0.5483231503961669\n",
      "At iteration 8000, loss = 0.5482268862601833\n",
      "At iteration 9000, loss = 0.5481356354814985\n",
      "At iteration 10000, loss = 0.5480491271629611\n",
      "At iteration 11000, loss = 0.5479671056695852\n",
      "At iteration 12000, loss = 0.5478893296569265\n",
      "At iteration 13000, loss = 0.5478155712371783\n",
      "At iteration 14000, loss = 0.5477456152022362\n",
      "At iteration 15000, loss = 0.5476792582936577\n",
      "At iteration 16000, loss = 0.5476163085159885\n",
      "At iteration 17000, loss = 0.5475565844908771\n",
      "At iteration 18000, loss = 0.5474999148493912\n",
      "At iteration 19000, loss = 0.5474461376602276\n",
      "At iteration 20000, loss = 0.547395099891682\n",
      "At iteration 21000, loss = 0.547346656905276\n",
      "At iteration 22000, loss = 0.5473006719791541\n",
      "At iteration 23000, loss = 0.5472570158594081\n",
      "At iteration 24000, loss = 0.5472155663377325\n",
      "At iteration 25000, loss = 0.5471762078536871\n",
      "At iteration 26000, loss = 0.5471388311202109\n",
      "At iteration 27000, loss = 0.5471033327709176\n",
      "At iteration 28000, loss = 0.5470696150279178\n",
      "At iteration 29000, loss = 0.5470375853888602\n"
     ]
    }
   ],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w, loss = reg_logistic_regression(y, tX_cleaned_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_cleaned_standardized = (remove_wrong_columns(tX_test)- tr_cleaned_mean)/tr_cleaned_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_cleaned_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
