{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 4]\n",
      " [2 1]\n",
      " [0 3]]\n",
      "(array([3, 2, 1, 4]), array([-1,  1,  1, -1]), array([6, 5]), array([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "y1 = np.array([1,1,-1,-1, 1, 1])\n",
    "x1 = np.array([1,2,3,4,5,6])\n",
    "k = 3\n",
    "k_indices = build_k_indices(y1, k, 23)\n",
    "print(k_indices)\n",
    "print(get_tr_and_te(y1, x1, k_indices, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    cost = 0.0\n",
    "    n = len(y)\n",
    "    for i in range(0, len(y)):\n",
    "        cost += (np.log(1 + np.exp(tx[i].T@w)) - y[i]*tx[i].T@w)/n\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T@(sigmoid(tx@w) - y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    \n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    w = w - gamma*gradient\n",
    "    \n",
    "    return loss, w\n",
    "\n",
    "def my_logistic_reg(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "            weights_k = np.zeros((k_fold))\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                #w_k, tr_loss_k = my_logistic_reg(tr_y_k, tr_tx_k, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            weight = np.mean(weights_k)\n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammas =  [1.e-05 1.e-06 1.e-07 1.e-08 1.e-09]\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "max_iters = 2500\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(5,10)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss = 0.6931471805599424\n",
      "At iteration 1000, loss = 1.121579901871062\n",
      "At iteration 2000, loss = 2.2034012683175748\n",
      "At iteration 0, loss = 1.119733951912261\n",
      "At iteration 1000, loss = 2.170528846816451\n",
      "At iteration 2000, loss = 3.5900982935283174\n",
      "At iteration 0, loss = 2.164408863360957\n",
      "At iteration 1000, loss = 3.59228204399471\n",
      "At iteration 2000, loss = 1.1180637840816787\n",
      "2.303868309007598\n",
      "188329.42507864253\n",
      "At iteration 0, loss = 3.899812979794199\n",
      "At iteration 1000, loss = 1.363073988541969\n",
      "At iteration 2000, loss = 2.282972025802477\n",
      "At iteration 0, loss = 1.3590409028983332\n",
      "At iteration 1000, loss = 2.2545735519186496\n",
      "At iteration 2000, loss = 4.00493039952471\n",
      "At iteration 0, loss = 2.2484722559278416\n",
      "At iteration 1000, loss = 4.01149020909986\n",
      "At iteration 2000, loss = 1.3535656551665716\n",
      "2.55122919993676\n",
      "206391.57946863945\n",
      "At iteration 0, loss = 4.88609634014767\n",
      "At iteration 1000, loss = 1.4834045476067004\n",
      "At iteration 2000, loss = 3.3010983438979813\n",
      "At iteration 0, loss = 1.479517938774186\n",
      "At iteration 1000, loss = 3.2521128340715655\n",
      "At iteration 2000, loss = 5.922390378886659\n",
      "At iteration 0, loss = 3.2483283812959423\n",
      "At iteration 1000, loss = 5.91883978142671\n",
      "At iteration 2000, loss = 1.4816290851254896\n",
      "3.5683851917233333\n",
      "262455.05417477485\n",
      "At iteration 0, loss = 4.972456463462031\n",
      "At iteration 1000, loss = 0.5717561537302223\n",
      "At iteration 2000, loss = 0.566333028488318\n",
      "At iteration 0, loss = 0.5645547958855358\n",
      "At iteration 1000, loss = 0.5628589510812299\n",
      "At iteration 2000, loss = 0.5620199430803419\n",
      "At iteration 0, loss = 0.562020288821137\n",
      "At iteration 1000, loss = 0.561692262384142\n",
      "At iteration 2000, loss = 0.561527040464483\n",
      "0.562691748061942\n",
      "46111.65263856768\n",
      "At iteration 0, loss = 0.6701786966017743\n",
      "At iteration 1000, loss = 0.5893929768387274\n",
      "At iteration 2000, loss = 0.5883917558984019\n",
      "At iteration 0, loss = 0.5878707905368664\n",
      "At iteration 0, loss = 0.5882175584881981\n",
      "0.5881522552569051\n",
      "48150.96442321127\n",
      "At iteration 0, loss = 0.68100858795091\n",
      "At iteration 0, loss = 0.6132734777478956\n",
      "At iteration 0, loss = 0.6136466745282458\n",
      "0.6135508285782032\n",
      "50171.34025715573\n",
      "At iteration 0, loss = 0.6023484997541558\n",
      "At iteration 1000, loss = 0.5870737778846279\n",
      "At iteration 2000, loss = 0.5826697448280601\n",
      "At iteration 0, loss = 0.580681740278657\n",
      "At iteration 1000, loss = 0.5786172465306745\n",
      "At iteration 2000, loss = 0.5770399540614314\n",
      "At iteration 0, loss = 0.5767277324637821\n",
      "At iteration 1000, loss = 0.5754908875601338\n",
      "At iteration 2000, loss = 0.5744059052110996\n",
      "0.5771813593949648\n",
      "47976.50875492025\n",
      "At iteration 0, loss = 0.5916070464858894\n",
      "At iteration 1000, loss = 0.5896619806963727\n",
      "At iteration 2000, loss = 0.5890013682583704\n",
      "At iteration 0, loss = 0.5883588383297466\n",
      "At iteration 1000, loss = 0.5881434534628396\n",
      "At iteration 2000, loss = 0.5880307334073318\n",
      "At iteration 0, loss = 0.5883404836239747\n",
      "At iteration 1000, loss = 0.5882887697482765\n",
      "At iteration 2000, loss = 0.5882597163810906\n",
      "0.5883590178726784\n",
      "48023.37592976342\n",
      "At iteration 0, loss = 0.6873446067146719\n",
      "At iteration 1000, loss = 0.6145402613436329\n",
      "At iteration 2000, loss = 0.6137495153101704\n",
      "At iteration 0, loss = 0.6132763432283503\n",
      "At iteration 0, loss = 0.6136480003235381\n",
      "0.6135527114984418\n",
      "50167.63744549725\n",
      "At iteration 0, loss = 0.6023151353036225\n",
      "At iteration 1000, loss = 0.5988997471790992\n",
      "At iteration 2000, loss = 0.5963647929624449\n",
      "At iteration 0, loss = 0.5947706733370387\n",
      "At iteration 1000, loss = 0.5929571750010574\n",
      "At iteration 2000, loss = 0.5914766919424499\n",
      "At iteration 0, loss = 0.5912468501591553\n",
      "At iteration 1000, loss = 0.5901050300422308\n",
      "At iteration 2000, loss = 0.589123081726028\n",
      "0.591610322227384\n",
      "49277.18838234839\n",
      "At iteration 0, loss = 0.5924852050745776\n",
      "At iteration 1000, loss = 0.5921405905575443\n",
      "At iteration 2000, loss = 0.5918382998558613\n",
      "At iteration 0, loss = 0.5911026108458103\n",
      "At iteration 1000, loss = 0.5908489080498913\n",
      "At iteration 2000, loss = 0.5906237175884966\n",
      "At iteration 0, loss = 0.5909248806507664\n",
      "At iteration 1000, loss = 0.5907258409717883\n",
      "At iteration 2000, loss = 0.5905490968928069\n",
      "0.5908960886984952\n",
      "48833.843797162095\n",
      "At iteration 0, loss = 0.6382765361747235\n",
      "At iteration 1000, loss = 0.6273848187590712\n",
      "At iteration 2000, loss = 0.6214004759536285\n",
      "At iteration 0, loss = 0.6190004054453213\n",
      "At iteration 1000, loss = 0.616559095044593\n",
      "At iteration 2000, loss = 0.6151859083280703\n",
      "At iteration 0, loss = 0.6150980713950625\n",
      "At iteration 1000, loss = 0.6145038458411961\n",
      "At iteration 2000, loss = 0.6141597551753336\n",
      "0.6160985576487418\n",
      "49693.059736618154\n",
      "At iteration 0, loss = 0.5999015647465225\n",
      "At iteration 1000, loss = 0.5995245689319153\n",
      "At iteration 2000, loss = 0.5991816422504437\n",
      "At iteration 0, loss = 0.5985311505487504\n",
      "At iteration 1000, loss = 0.5982013308971627\n",
      "At iteration 2000, loss = 0.5978837176697392\n",
      "At iteration 0, loss = 0.598106783126862\n",
      "At iteration 1000, loss = 0.5978053758794813\n",
      "At iteration 2000, loss = 0.5975137963978351\n",
      "0.5980392103524864\n",
      "49823.19543210779\n",
      "At iteration 0, loss = 0.599089660864476\n",
      "At iteration 1000, loss = 0.598886250621154\n",
      "At iteration 2000, loss = 0.5986893719218687\n",
      "At iteration 0, loss = 0.5980843093472789\n",
      "At iteration 1000, loss = 0.5978915549651462\n",
      "At iteration 2000, loss = 0.5977052116838921\n",
      "At iteration 0, loss = 0.5979968707544251\n",
      "At iteration 1000, loss = 0.5978194856193416\n",
      "At iteration 2000, loss = 0.5976482511791131\n",
      "0.5979242456666257\n",
      "49664.663319556916\n",
      "At iteration 0, loss = 0.616070813046643\n",
      "At iteration 1000, loss = 0.6159071707736159\n",
      "At iteration 2000, loss = 0.6157678095495602\n",
      "At iteration 0, loss = 0.6151950995729567\n",
      "At iteration 1000, loss = 0.6150772193360361\n",
      "At iteration 2000, loss = 0.6149672663001337\n",
      "At iteration 0, loss = 0.6152863060662328\n",
      "At iteration 1000, loss = 0.6151855444696661\n",
      "At iteration 2000, loss = 0.6150922303438208\n",
      "0.6152221204792555\n",
      "49695.29474786993\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "#w, loss = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change lambdas and gammas seeing this result to reduce computation time\n",
    "before best was \n",
    "gamma =  1e-06  lambda =  100\n",
    "At iteration 6000, loss = 0.5555634390152187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammas =  [1.e-06 1.e-07]\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "max_iters = 3000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000])\n",
    "gammas = np.array([10**(-i) for i in range(6,8)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss = 0.6931471805599424\n",
      "At iteration 1000, loss = 0.5684430043539948\n",
      "At iteration 2000, loss = 0.5583692247724278\n",
      "At iteration 0, loss = 0.5511264311615018\n",
      "At iteration 1000, loss = 0.5460653088954509\n",
      "At iteration 2000, loss = 0.5422474944368155\n",
      "At iteration 0, loss = 0.5394698316506356\n",
      "At iteration 1000, loss = 0.5371208207547129\n",
      "At iteration 2000, loss = 0.5352654732448939\n",
      "0.5414543076955408\n",
      "45091.577688956146\n",
      "At iteration 0, loss = 0.5370512202125811\n",
      "At iteration 1000, loss = 0.5364209571518908\n",
      "At iteration 2000, loss = 0.5359194013519164\n",
      "At iteration 0, loss = 0.5357680009964346\n",
      "At iteration 1000, loss = 0.5354509790411728\n",
      "At iteration 2000, loss = 0.5351932177944126\n",
      "At iteration 0, loss = 0.5350665959556371\n",
      "At iteration 1000, loss = 0.534877907388126\n",
      "At iteration 2000, loss = 0.5347233412289472\n",
      "0.5350280068978794\n",
      "44142.23929225857\n",
      "At iteration 0, loss = 0.5866304758119318\n",
      "At iteration 1000, loss = 0.574730786235232\n",
      "At iteration 2000, loss = 0.5684202081603393\n",
      "At iteration 0, loss = 0.5651287933612793\n",
      "At iteration 1000, loss = 0.5632774313471997\n",
      "At iteration 2000, loss = 0.5622962789955954\n",
      "At iteration 0, loss = 0.561978578972674\n",
      "At iteration 1000, loss = 0.5616957254329297\n",
      "At iteration 2000, loss = 0.5615433122178519\n",
      "0.5627696839663471\n",
      "45206.79040552307\n",
      "At iteration 0, loss = 0.7013432344789441\n",
      "At iteration 1000, loss = 0.5898339010894509\n",
      "At iteration 2000, loss = 0.5883980665868284\n",
      "At iteration 0, loss = 0.5878706328093243\n",
      "At iteration 0, loss = 0.5882176202214217\n",
      "0.5881522672408991\n",
      "48151.174581485444\n",
      "At iteration 0, loss = 0.5781013986914636\n",
      "At iteration 1000, loss = 0.5751928187416517\n",
      "At iteration 2000, loss = 0.5732137295266682\n",
      "At iteration 0, loss = 0.5711065322549079\n",
      "At iteration 1000, loss = 0.5696994079172696\n",
      "At iteration 2000, loss = 0.568408863067738\n",
      "At iteration 0, loss = 0.5675392206767453\n",
      "At iteration 1000, loss = 0.5663784170023537\n",
      "At iteration 2000, loss = 0.5652761056856023\n",
      "0.5676675770989607\n",
      "47299.63229514132\n",
      "At iteration 0, loss = 0.5646107452923562\n",
      "At iteration 1000, loss = 0.5636325570785344\n",
      "At iteration 2000, loss = 0.5626984323269585\n",
      "At iteration 0, loss = 0.5614995418429929\n",
      "At iteration 1000, loss = 0.5606636124136273\n",
      "At iteration 2000, loss = 0.5598624825534152\n",
      "At iteration 0, loss = 0.5593891988653916\n",
      "At iteration 1000, loss = 0.5586335062109851\n",
      "At iteration 2000, loss = 0.5579095886888801\n",
      "0.5593694671501261\n",
      "46561.75119352859\n",
      "At iteration 0, loss = 0.5636731329973643\n",
      "At iteration 1000, loss = 0.5634899568134148\n",
      "At iteration 2000, loss = 0.5633331198260721\n",
      "At iteration 0, loss = 0.5629794580928594\n",
      "At iteration 1000, loss = 0.562852757579079\n",
      "At iteration 2000, loss = 0.5627389637020143\n",
      "At iteration 0, loss = 0.5628979536073923\n",
      "At iteration 1000, loss = 0.5627916902690071\n",
      "At iteration 2000, loss = 0.5626969017670104\n",
      "0.5628114022181784\n",
      "46218.92016923975\n",
      "At iteration 0, loss = 0.6404190075313767\n",
      "At iteration 1000, loss = 0.6209394269409043\n",
      "At iteration 2000, loss = 0.6090932848730942\n",
      "At iteration 0, loss = 0.601320317075298\n",
      "At iteration 1000, loss = 0.596515372404462\n",
      "At iteration 2000, loss = 0.5934391048182723\n",
      "At iteration 0, loss = 0.5917743226788691\n",
      "At iteration 1000, loss = 0.5905090672112355\n",
      "At iteration 2000, loss = 0.5896956210086127\n",
      "0.5940966313989348\n",
      "47525.07009379773\n"
     ]
    }
   ],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma =  1e-06  lambda =  10\n",
      "At iteration 0, loss = 0.5328341757248475\n",
      "At iteration 1000, loss = 0.532452677185637\n",
      "At iteration 2000, loss = 0.5321516183711678\n",
      "At iteration 3000, loss = 0.531909762256691\n",
      "At iteration 4000, loss = 0.5317122514246776\n",
      "At iteration 5000, loss = 0.5315485407960628\n",
      "At iteration 6000, loss = 0.5314110377112972\n",
      "At iteration 7000, loss = 0.5312941953740249\n",
      "At iteration 8000, loss = 0.5311939013393535\n",
      "At iteration 9000, loss = 0.5311070605521278\n",
      "At iteration 10000, loss = 0.531031308152806\n",
      "At iteration 11000, loss = 0.5309648097321319\n",
      "At iteration 12000, loss = 0.5309061210691471\n",
      "At iteration 13000, loss = 0.5308540886833469\n",
      "At iteration 14000, loss = 0.5308077786240801\n",
      "At iteration 15000, loss = 0.5307664249543416\n",
      "At iteration 16000, loss = 0.5307293920817708\n",
      "At iteration 17000, loss = 0.5306961469057752\n",
      "At iteration 18000, loss = 0.5306662379823048\n",
      "At iteration 19000, loss = 0.5306392797501206\n",
      "At iteration 20000, loss = 0.53061494044167\n",
      "At iteration 21000, loss = 0.5305929327025244\n",
      "At iteration 22000, loss = 0.5305730062223469\n",
      "At iteration 23000, loss = 0.5305549418757505\n",
      "At iteration 24000, loss = 0.5305385470090548\n",
      "At iteration 25000, loss = 0.5305236516065505\n",
      "At iteration 26000, loss = 0.530510105139514\n",
      "At iteration 27000, loss = 0.5304977739512655\n",
      "At iteration 28000, loss = 0.5304865390677854\n"
     ]
    }
   ],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w, loss = reg_logistic_regression(y, tX_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_standardized = (tX_test- tr_mean)/tr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
