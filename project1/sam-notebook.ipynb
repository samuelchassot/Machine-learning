{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array([1,1,-1,-1, 1, 1])\n",
    "x1 = np.array([1,2,3,4,5,6])\n",
    "k = 3\n",
    "k_indices = build_k_indices(y1, k, 23)\n",
    "print(k_indices)\n",
    "print(get_tr_and_te(y1, x1, k_indices, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "\n",
    "np.polynomial.polynomial.polyvander(x, 5).T\n",
    "expand_features_polynomial(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    cost = 0.0\n",
    "    n = len(y)\n",
    "    for i in range(0, len(y)):\n",
    "        cost += (np.log(1 + np.exp(tx[i].T@w)) - y[i]*tx[i].T@w)/n\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T@(sigmoid(tx@w) - y)\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descen using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    \n",
    "    gradient = calculate_gradient(y, tx, w)\n",
    "    \n",
    "    w = w - gamma*gradient\n",
    "    \n",
    "    return loss, w\n",
    "\n",
    "def my_logistic_reg(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "            weights_k = np.zeros((k_fold))\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                #w_k, tr_loss_k = my_logistic_reg(tr_y_k, tr_tx_k, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            weight = np.mean(weights_k)\n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "max_iters = 2500\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(5,10)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "#w, loss = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change lambdas and gammas seeing this result to reduce computation time\n",
    "before best was \n",
    "gamma =  1e-06  lambda =  100\n",
    "At iteration 6000, loss = 0.5555634390152187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "max_iters = 3000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000])\n",
    "gammas = np.array([10**(-i) for i in range(6,8)])\n",
    "print(\"gammas = \", gammas)\n",
    "w_initial = np.array([0.0 for i in range(tX.shape[1])])\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w, loss = reg_logistic_regression(y, tX_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best with regularized logistic regression\n",
    "0.737\n",
    "\n",
    "gamma =  1e-06  lambda =  10\n",
    "At iteration 28000, loss = 0.5304865390677854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_standardized = (tX_test- tr_mean)/tr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_losses_gamma_10e6 = np.array(list(zip(te_losses[0], lambdas_)))\n",
    "te_losses_gamma_10e7 = np.array(list(zip(te_losses[1], lambdas_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([te_losses_gamma_10e6[i][1] for i in range(len(te_losses_gamma_10e6))], [te_losses_gamma_10e6[i][0] for i in range(len(te_losses_gamma_10e6))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_losses_gamma_10e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try with cleaned data\n",
    "\n",
    "it is also the clean path for my part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y\n",
    "\n",
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "            weights_k = np.zeros((k_fold))\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            weight = np.mean(weights_k)\n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "max_iters = 3000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(6,10)])\n",
    "\n",
    "tX_cleaned_standardized, tr_cleaned_mean, tr_cleaned_std = standardize(remove_wrong_columns(tX))\n",
    "w_initial = np.array([0.0 for i in range(tX_cleaned_standardized.shape[1])])\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_cleaned_standardized, w_initial, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w, loss = reg_logistic_regression(y, tX_cleaned_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_cleaned_standardized = (remove_wrong_columns(tX_test)- tr_cleaned_mean)/tr_cleaned_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_cleaned_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(lambdas_, te_losses[0], marker=\".\", color='b', label='test error with gamma={}'.format(gammas[0]))\n",
    "plt.semilogx(lambdas_, te_losses[1], marker=\".\", color='r', label='test error with gamma={}'.format(gammas[1]))\n",
    "plt.semilogx(lambdas_, te_losses[2], marker=\".\", color='g', label='test error with gamma={}'.format(gammas[2]))\n",
    "plt.semilogx(lambdas_, te_losses[3], marker=\".\", color='y', label='test error with gamma={}'.format(gammas[3]))\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"regularized logistic regression\")\n",
    "plt.legend(loc=2)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Code to copy for final project\n",
    "it computes with regularized logistic regression for raw and clean data, and makes a plot of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def get_tr_and_te(y, tx, k_indices, k):\n",
    "    te_y = y[k_indices[k]]\n",
    "    te_tx = tx[k_indices[k]]\n",
    "    tr_indices = []\n",
    "    for i, indices in zip(range(len(k_indices)), k_indices):\n",
    "        if i != k:\n",
    "            tr_indices.append(indices)\n",
    "            \n",
    "    tr_indices = np.array(tr_indices).flatten()\n",
    "    return tx[tr_indices], y[tr_indices], te_tx, te_y\n",
    "\n",
    "def cross_validation_reg_log_regr(y, tx, w_initial, max_iters, gammas, lambdas_, k_fold, seed):\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    tr_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "    te_losses = np.zeros((len(gammas), len(lambdas_)))\n",
    "\n",
    "    for gamma_index,gamma in zip(range(len(gammas)), gammas):\n",
    "        for lambda_index, lambda_ in zip(range(len(lambdas_)),lambdas_):\n",
    "            tr_k_losses = np.zeros((k_fold))\n",
    "            te_k_losses = np.zeros((k_fold))\n",
    "\n",
    "            for k in range(k_fold):\n",
    "                tr_tx_k, tr_y_k, te_tx_k, te_y_k = get_tr_and_te(y, tx, k_indices, k)\n",
    "                \n",
    "                w_k, tr_loss_k = reg_logistic_regression(tr_y_k, tr_tx_k, lambda_, w_initial, max_iters, gamma)\n",
    "                \n",
    "                te_loss_k = calculate_loss_sigmoid(te_y_k, te_tx_k, w_k)\n",
    "                \n",
    "                tr_k_losses[k] = tr_loss_k\n",
    "                te_k_losses[k] = te_loss_k\n",
    "                \n",
    "            tr_loss = np.mean(tr_k_losses)/(k_fold-1)     #NOT SURE\n",
    "            te_loss = np.mean(te_k_losses)\n",
    "            \n",
    "            tr_losses[gamma_index][lambda_index] = tr_loss\n",
    "            te_losses[gamma_index][lambda_index] = te_loss\n",
    "            \n",
    "            print(tr_loss)\n",
    "            print(te_loss)\n",
    "            argmin = np.argmin(te_losses)\n",
    "            gamma_idx = argmin//len(lambdas_)\n",
    "            lambda_idx = argmin%len(lambdas_)\n",
    "            gamma = gammas[gamma_idx]\n",
    "            lambda_ = lambdas_[lambda_idx]\n",
    "\n",
    "    return tr_losses, te_losses, gamma, lambda_\n",
    "\n",
    "def make_plots_reg_log_regr(te_losses, lambdas_, gammas, save_name, title):\n",
    "    plt.semilogx(lambdas_, te_losses[0], marker=\".\", color='b', label='test error with gamma={}'.format(gammas[0]))\n",
    "    plt.semilogx(lambdas_, te_losses[1], marker=\".\", color='r', label='test error with gamma={}'.format(gammas[1]))\n",
    "    plt.semilogx(lambdas_, te_losses[2], marker=\".\", color='g', label='test error with gamma={}'.format(gammas[2]))\n",
    "    plt.semilogx(lambdas_, te_losses[3], marker=\".\", color='y', label='test error with gamma={}'.format(gammas[3]))\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables for RAW DATA and CLEANED DATA\n",
    "max_iters = 3000\n",
    "k_fold = 3\n",
    "seed = 142\n",
    "lambdas_ = np.array([1, 10, 100, 1000, 10000])\n",
    "gammas = np.array([10**(-i) for i in range(6,10)])\n",
    "\n",
    "tX_standardized, tr_mean, tr_std = standardize(tX)\n",
    "tX_cleaned_standardized, tr_cleaned_mean, tr_cleaned_std = standardize(remove_wrong_columns(tX))\n",
    "\n",
    "\n",
    "w_initial_raw = np.array([0.0 for i in range(tX_standardized.shape[1])])\n",
    "\n",
    "w_initial_clean = np.array([0.0 for i in range(tX_cleaned_standardized.shape[1])])\n",
    "\n",
    "#to work with loss\n",
    "y = np.array([int((y[i] + 1)/2) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses, te_losses, gamma, lambda_ = cross_validation_reg_log_regr(y, tX_standardized, w_initial_raw, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_losses_clean, te_losses_clean, gamma_clean, lambda_clean = cross_validation_reg_log_regr(y, tX_cleaned_standardized, w_initial_clean, max_iters, gammas, lambdas_, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w_clean, loss_clean = reg_logistic_regression(y, tX_cleaned_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gamma = \", gamma, \" lambda = \", lambda_)\n",
    "w_raw, loss_raw = reg_logistic_regression(y, tX_cleaned_standardized, lambda_, w_initial, 30000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots_reg_log_regr(te_losses, lambdas_, gammas, \"raw_data_reg_log_regr\", \"regularized logistic regression raw data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots_reg_log_regr(te_losses_clean, lambdas_, gammas, \"clean_data_reg_log_regr\", \"regularized logistic regression cleaned data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_cleaned_standardized = (remove_wrong_columns(tX_test)- tr_cleaned_mean)/tr_cleaned_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/output.csv' \n",
    "y_pred = predict_labels(w, tX_test_cleaned_standardized)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
