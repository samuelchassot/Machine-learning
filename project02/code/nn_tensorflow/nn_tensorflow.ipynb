{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and instruction to run\n",
    "\n",
    "This notebook is the notebook used for neural nets training on Google Colab. To run it, you will need to install tensorflow. Moreover, loading the stanford embedding takes around 6 hours and computing the vectors for the full dataset takes around 10 hours. This is why there some savings of numpy arrays as checkpoints.\n",
    "\n",
    "To run the notebook, you need data that was too big to be given back with the rest. We then uploaded to our Google Drive and here is the link to download it:\n",
    "\n",
    "https://drive.google.com/drive/folders/1r8tVqsL2PJ8VaUk7AHsEbeiwoqkxO3Qc?usp=sharing\n",
    "\n",
    "\n",
    "You need to extract the two folders that are in the archive ('data' and 'glove_from_stanford') beside this notebook's file.\n",
    "\n",
    "To prove that we haven't modified the archive since the deadline here is the hash (md5) from the .zip file:\n",
    "\n",
    "38c149f74806e5c65288825267ecd12a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWlgkBnq0nqY"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def words_list(file_name):\n",
    "    words_list = []\n",
    "    f = open(file_name, \"r\")\n",
    "    for l in f.readlines():\n",
    "        l = l.strip()\n",
    "        words_list.append(l)\n",
    "    words_list = np.array(words_list)\n",
    "    f.close()\n",
    "    return words_list\n",
    "\n",
    "def tweets_txt(file_name):\n",
    "    tweets_txt = []\n",
    "    f = open(file_name, \"r\")\n",
    "    for l in f.readlines():\n",
    "        tweets_txt.append(l.strip())\n",
    "    f.close()\n",
    "    return np.array(tweets_txt)\n",
    "\n",
    "def tweet_means(tweets_txt, word_embeddings, words_list, embedding_size):\n",
    "    tweets_vec = []\n",
    "    i = 0\n",
    "    for tw in tweets_txt:\n",
    "        words_in_tweet = tw.split(\" \")\n",
    "        acc = np.zeros(embedding_size)\n",
    "        for w in words_in_tweet:\n",
    "            vec = word_embeddings[np.argmax(words_list==w)]\n",
    "            acc += vec\n",
    "        acc = acc/len(words_in_tweet)\n",
    "        tweets_vec.append(acc)\n",
    "        if i%1000 == 0:\n",
    "            print(i, \" done\")\n",
    "        i += 1\n",
    "    tweets_vec = np.array(tweets_vec)\n",
    "    return tweets_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tv4Bw-T1ExI"
   },
   "source": [
    "# Imports for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('No GPU Found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRKdN5K_sTZH"
   },
   "source": [
    "# Load GloVe embedding from Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"glove_from_stanford/glove.twitter.27B.200d.txt\", \"r\")\n",
    "words = []\n",
    "\n",
    "i = 0\n",
    "embeddings = []\n",
    "for l in f.readlines():\n",
    "    li = l.split()\n",
    "    w = li[0]\n",
    "    vec_string = li[1:]\n",
    "    vec = []\n",
    "    for e in vec_string:\n",
    "        vec.append(float(e))\n",
    "    vec = np.array(vec)\n",
    "    if i%10000 == 0:\n",
    "        print(\"done: \", i )\n",
    "    if vec.shape[0] == 200:\n",
    "        words.append(w)\n",
    "        embeddings.append(vec)\n",
    "\n",
    "    else:\n",
    "        print(w, \" was not the right shape. The shape was: \", vec.shape)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XGaB9g5sTZM"
   },
   "outputs": [],
   "source": [
    "embedding_stacked = np.stack(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKkVyxzgsTZO"
   },
   "outputs": [],
   "source": [
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RjLVbzesTZR"
   },
   "outputs": [],
   "source": [
    "np.save(\"embedding_stanford.npy\", embedding_stacked)\n",
    "np.save(\"words_stanford.npy\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wi8vdJX5ixu"
   },
   "source": [
    "# Reduce stanford embedding for this particular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/vocab_cut_clean_kim_yoon.txt\", 'r')\n",
    "words = []\n",
    "for l in f.readlines():\n",
    "    words.append(l[:-1])\n",
    "words = np.array(words)\n",
    "f.close()\n",
    "np.save(\"words_full_list_clean_kim_yoon.npy\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_stanford = np.load(\"embedding_stanford.npy\")\n",
    "word_list_stanford = np.load(\"words_stanford.npy\")\n",
    "\n",
    "words_list_full_dataset = np.load(\"words_full_list_clean_kim_yoon.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lp6Ha0HQ56qL"
   },
   "outputs": [],
   "source": [
    "words_needed = np.isin(word_list_stanford, words_list_full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxmFeYTK6pkm"
   },
   "outputs": [],
   "source": [
    "word_wanted_indices = np.nonzero(words_needed*1.)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25j6MqxQ8NOk"
   },
   "outputs": [],
   "source": [
    "reduced_embedding_stanford = embedding_stanford[word_wanted_indices]\n",
    "reduced_words_stanford = word_list_stanford[word_wanted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FllJKRpo3gwK"
   },
   "source": [
    "# Compute vectors for tweets FULL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GB1CUa9x4gCw"
   },
   "outputs": [],
   "source": [
    "embedding = reduced_embedding_stanford\n",
    "word_list = reduced_words_stanford\n",
    "n_features = embedding.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_full_txt = []\n",
    "f = open(\"data/train_pos_full_clean_kim_yoon.txt\")\n",
    "for l in f.readlines():\n",
    "  tweets_pos_full_txt.append(l.strip())\n",
    "tweets_pos_full_txt = np.array(tweets_pos_full_txt)\n",
    "tweets_vecs_pos_full = tweet_means(tweets_pos_full_txt, embedding, word_list, n_features)\n",
    "np.save(\"tweets_pos_full_clean_stanford.npy\", tweets_vecs_pos_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_neg_full_txt = []\n",
    "f = open(\"data/train_neg_full_clean_kim_yoon.txt\")\n",
    "for l in f.readlines():\n",
    "    tweets_neg_full_txt.append(l.strip())\n",
    "tweets_neg_full_txt = np.array(tweets_neg_full_txt)\n",
    "tweets_vecs_neg_full = tweet_means(tweets_neg_full_txt, embedding, word_list, n_features)\n",
    "np.save(\"tweets_neg_full_clean_stanford.npy\", tweets_vecs_neg_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyxGw7rCCfKg"
   },
   "source": [
    "\n",
    "# Load saved data FULL (FROM HERE to test models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25628,
     "status": "ok",
     "timestamp": 1576481560073,
     "user": {
      "displayName": "Samuel Chassot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64",
      "userId": "17892117135253630625"
     },
     "user_tz": -60
    },
    "id": "TZtasSvK4nR6",
    "outputId": "11ef569c-9258-4c6e-aff0-b0c3e368d022"
   },
   "outputs": [],
   "source": [
    "tweets_pos = np.load(\"tweets_pos_full_clean_stanford.npy\")\n",
    "tweets_neg = np.load(\"tweets_neg_full_clean_stanford.npy\")\n",
    "X = np.vstack((tweets_pos, tweets_neg))\n",
    "y = np.array([1 for i in range(tweets_pos.shape[0])] + [-1 for i in range(tweets_neg.shape[0])])\n",
    "\n",
    "indices = np.random.permutation([i for i in range(y.shape[0])])\n",
    "\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#because of protobuf limit of 2GB\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "n_features=X.shape[1]\n",
    "\n",
    "j = 0.9\n",
    "\n",
    "train_X = X[0:int(j*len(y))]\n",
    "train_y = y[0:int(j*len(y))]\n",
    "test_X = X[int(j*len(y)):]\n",
    "test_y = y[int(j*len(y)):]\n",
    "\n",
    "train_y[train_y == -1.] = 0.\n",
    "test_y[test_y == -1.] = 0.\n",
    "\n",
    "#free some ram\n",
    "tweets_pos = 0.0\n",
    "tweets_neg = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrSCx74MCoyc"
   },
   "source": [
    "# Work with data GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgH_Ed-PsTZh"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(n_features))\n",
    "model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.05))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "model.save('models/model_temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vaf2wYnCsTZf"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixDEqZvnJcCk"
   },
   "source": [
    "Here I often save the model and reload it because the Colab notebook experienced some crash so with this technique, we do not loose to much work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6486,
     "status": "error",
     "timestamp": 1576577701803,
     "user": {
      "displayName": "Samuel Chassot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64",
      "userId": "17892117135253630625"
     },
     "user_tz": -60
    },
    "id": "-Hh7yxbxIQIB",
    "outputId": "315617da-d6c5-4d4e-d6ee-3220497addd3"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "for _ in range(15):\n",
    "  model = tf.keras.models.load_model('models/model_temp-1.h5')\n",
    "  model.fit(train_dataset, epochs=3)\n",
    "  f = open(\"models/number-of-it-done-on-model-temp-1.txt\", 'a')\n",
    "  f.write(str(n_epochs) + '\\n')\n",
    "  f.close()\n",
    "  model.save('/content/drive/My Drive/Colab Notebooks/ML-MA1/Project02/models/model_temp-1.h5')\n",
    "  model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14896,
     "status": "ok",
     "timestamp": 1576593289957,
     "user": {
      "displayName": "Samuel Chassot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64",
      "userId": "17892117135253630625"
     },
     "user_tz": -60
    },
    "id": "xK_qIneKKBI9",
    "outputId": "72c0b469-3a06-4c90-ae6c-b21b9a12a230"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/ML-MA1/Project02/models/model_temp-1.h5')\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiHbrf1TM3hn"
   },
   "source": [
    "# Load test data and output for AIcrowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117657,
     "status": "ok",
     "timestamp": 1576245199724,
     "user": {
      "displayName": "Samuel Chassot",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64",
      "userId": "17892117135253630625"
     },
     "user_tz": -60
    },
    "id": "gcnwqKa5sTZt",
    "outputId": "a24d0fd2-1697-4cab-8c42-53d8684a0ad1"
   },
   "outputs": [],
   "source": [
    "embedding = reduced_embedding_stanford\n",
    "word_list = reduced_words_stanford\n",
    "n_features = embedding.shape[1]\n",
    "\n",
    "tweets_test_txt = []\n",
    "f = open(\"data/test_data_clean_kim_yoon.txt\")\n",
    "for l in f.readlines():\n",
    "    l = l.strip()\n",
    "    l = l[l.find(',')+1:]\n",
    "    tweets_test_txt.append(l.strip())\n",
    "tweets_test_txt = np.array(tweets_test_txt)\n",
    "print(\"begin means\")\n",
    "tweets_vecs_test = tweet_means(tweets_test_txt, embedding, word_list, n_features)\n",
    "\n",
    "np.save(\"tweets_test_clean_stanford.npy\", tweets_vecs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSpdx5BbgW08"
   },
   "source": [
    "# Make CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SR1QUfqbZxU"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/model_temp.h5')\n",
    "tweets_vecs_test = np.load(\"tweets_test_clean_stanford.npy\")\n",
    "y = model.predict(tweets_vecs_test)\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "# make csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "  f.write(\"Id,Prediction\\n\")\n",
    "  id = 1\n",
    "  for i in y:\n",
    "    if i == 0:\n",
    "      i = -1\n",
    "    l = str(id) + \",\" + str(i) + \"\\n\"\n",
    "    f.write(l)\n",
    "    id = id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AWlgkBnq0nqY",
    "vRKdN5K_sTZH",
    "7wi8vdJX5ixu",
    "edPmrZmLsTZU",
    "FllJKRpo3gwK",
    "8GqoxXIPUjsY",
    "7TErR5mKyXAD",
    "AiHbrf1TM3hn"
   ],
   "machine_shape": "hm",
   "name": "sam_tensorflow_bis_model-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
