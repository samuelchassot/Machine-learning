{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Tweets Classification\n",
    "Source : https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358\n",
    "\n",
    "You need ktrain, keras and tensorflow to use this notebook. It is strongly adviced to either use Google Colab or either set tensorflow to run it on a good GPU.\n",
    "\n",
    "As this uses a lot of RAM and processing time, the obtained result on AICrowd where made on the small training set of tweets. If you have a powerful computer, we advice you to train it on the full set to have better results.\n",
    "\n",
    "This is our best submission on AICrowd with accuraca : 0.876, F-1 score : 0.879."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_test_train(tweets):\n",
    "    \"\"\"Divide an array of tweet into two parts, one containing 75% and the other one, 25%\"\"\"\n",
    "    index = int(0.75 * len(tweets))\n",
    "    return tweets[:index], tweets[index:]\n",
    "\n",
    "def tweets_txt(file_name):\n",
    "    \"\"\"Parse txt files to obtain an array of tweets and remove duplicates\"\"\"\n",
    "    tweets_txt = []\n",
    "    f = open(file_name, \"r\")\n",
    "    for l in f.readlines():\n",
    "        tweets_txt.append(l.strip())\n",
    "    f.close()\n",
    "    return np.array(list(set(tweets_txt)))\n",
    "\n",
    "def tweets_txt_test(file_name):\n",
    "    \"\"\"Parse a txt file and return an array of tweets without removing duplicates\"\"\"\n",
    "    tweets_txt = []\n",
    "    f = open(file_name, \"r\")\n",
    "    for l in f.readlines():\n",
    "        tweets_txt.append(l.strip())\n",
    "    f.close()\n",
    "    return np.array(tweets_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare folder for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the folder, you need to create this structure in you current directory :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current Directory\n",
    " ├── BERT_folder\n",
    " │   ├── train\n",
    " │   │   ├── pos       \n",
    " |   │   ├── neg\n",
    " |   |  \n",
    " │   ├── test\n",
    " │   │   ├── pos       \n",
    " |   │   ├── neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = tweets_txt(\"Datasets/twitter-datasets/train_pos.txt\")\n",
    "tweets_neg = tweets_txt(\"Datasets/twitter-datasets/train_neg.txt\")\n",
    "\n",
    "tweets_pos_train, tweets_pos_test = divide_test_train(tweets_pos)\n",
    "tweets_neg_train, tweets_neg_test = divide_test_train(tweets_neg)\n",
    "\n",
    "i = 0\n",
    "for t in tweets_pos_train:\n",
    "    f= open(\"BERT_folder/train/pos/%d.txt\" %i,\"w+\")\n",
    "    f.write(t)\n",
    "    f.close()\n",
    "    i+=1\n",
    "print(\"DONE pos train\")\n",
    "\n",
    "i = 0\n",
    "for t in tweets_pos_test:\n",
    "    f= open(\"BERT_folder/test/pos/%d.txt\" %i,\"w+\")\n",
    "    f.write(t)\n",
    "    f.close()\n",
    "    i+=1\n",
    "\n",
    "print(\"DONE pos test\")\n",
    "\n",
    "i = 0\n",
    "for t in tweets_neg_train:\n",
    "    f= open(\"BERT_folder/train/neg/%d.txt\" %i,\"w+\")\n",
    "    f.write(t)\n",
    "    f.close()\n",
    "    i+=1\n",
    "\n",
    "print(\"DONE neg train\")\n",
    "\n",
    "i = 0\n",
    "for t in tweets_neg_test:\n",
    "    f= open(\"BERT_folder/test/neg/%d.txt\" %i,\"w+\")\n",
    "    f.write(t)\n",
    "    f.close()\n",
    "    i+=1\n",
    "\n",
    "print(\"DONE neg test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maxlen of 199 comes from the CNN part, where after reduction, we saw that only 199 tokens at most where useful.\n",
    "Since this algorithm is time consuming, being able to reduce maxlen is a good way to gain time. Furthermore, using Google Colab, we tried not constraining maxlen but it gave worse result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_small, y_train_small), (x_test_small, y_test_small), preproc_small = text.texts_from_folder(\"BERT_folder\", \n",
    "                                                                       maxlen=199, \n",
    "                                                                       preprocess_mode='bert',\n",
    "                                                                       train_test_names=['train', \n",
    "                                                                                         'test'],\n",
    "                                                                       classes=['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = text.text_classifier('bert', (x_train_small, y_train_small), preproc=preproc_small)\n",
    "learner_small = ktrain.get_learner(model_small,train_data=(x_train_small, y_train_small), val_data=(x_test_small, y_test_small), batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper on BERT and the tutorial we followed mention 2e-5 as a good learning rate so we used it.\n",
    "\n",
    "Since it takes a long time, we tried only 1 epoch with this model. With a more constrained model (maxlen = 128 and batch_size = 32), we tried 3 epochs but the result was not better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_small.fit_onecycle(2e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner_small.model, preproc_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = tweets_txt_test(\"Datasets/twitter-datasets/test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    id = 1\n",
    "    for i in result:\n",
    "        if i == \"neg\":\n",
    "            i = -1\n",
    "        if i == \"pos\":\n",
    "            i = 1\n",
    "        l = str(id) + \",\" + str(i) + \"\\n\"\n",
    "        f.write(l)\n",
    "        id = id + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}