{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import fasttext as ft\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe embedding from Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:  0\n",
      "done:  10000\n",
      "done:  20000\n",
      "done:  30000\n",
      "-0.29736  was not the right shape. The shape was:  (199,)\n",
      "done:  40000\n",
      "done:  50000\n",
      "done:  60000\n",
      "done:  70000\n",
      "done:  80000\n",
      "done:  90000\n",
      "done:  100000\n",
      "done:  110000\n",
      "done:  120000\n",
      "done:  130000\n",
      "done:  140000\n",
      "done:  150000\n",
      "done:  160000\n",
      "done:  170000\n",
      "done:  180000\n",
      "done:  190000\n",
      "done:  200000\n",
      "done:  210000\n",
      "done:  220000\n",
      "done:  230000\n",
      "done:  240000\n",
      "done:  250000\n",
      "done:  260000\n",
      "done:  270000\n",
      "done:  280000\n",
      "done:  290000\n",
      "done:  300000\n",
      "done:  310000\n",
      "done:  320000\n",
      "done:  330000\n",
      "done:  340000\n",
      "done:  350000\n",
      "done:  360000\n",
      "done:  370000\n",
      "done:  380000\n",
      "done:  390000\n",
      "done:  400000\n",
      "done:  410000\n",
      "done:  420000\n",
      "done:  430000\n",
      "done:  440000\n",
      "done:  450000\n",
      "done:  460000\n",
      "done:  470000\n",
      "done:  480000\n",
      "done:  490000\n",
      "done:  500000\n",
      "done:  510000\n",
      "done:  520000\n",
      "done:  530000\n",
      "done:  540000\n",
      "done:  550000\n",
      "done:  560000\n",
      "done:  570000\n",
      "done:  580000\n",
      "done:  590000\n",
      "done:  600000\n",
      "done:  610000\n",
      "done:  620000\n",
      "done:  630000\n",
      "done:  640000\n",
      "done:  650000\n",
      "done:  660000\n",
      "done:  670000\n",
      "done:  680000\n",
      "done:  690000\n",
      "done:  700000\n",
      "done:  710000\n",
      "done:  720000\n",
      "done:  730000\n",
      "done:  740000\n",
      "done:  750000\n",
      "done:  760000\n",
      "done:  770000\n",
      "done:  780000\n",
      "done:  790000\n",
      "done:  800000\n",
      "done:  810000\n",
      "done:  820000\n",
      "done:  830000\n",
      "done:  840000\n",
      "done:  850000\n",
      "done:  860000\n",
      "done:  870000\n",
      "done:  880000\n",
      "done:  890000\n",
      "done:  900000\n",
      "done:  910000\n",
      "done:  920000\n",
      "done:  930000\n",
      "done:  940000\n",
      "done:  950000\n",
      "done:  960000\n",
      "done:  970000\n",
      "done:  980000\n",
      "done:  990000\n",
      "done:  1000000\n",
      "done:  1010000\n",
      "done:  1020000\n",
      "done:  1030000\n",
      "done:  1040000\n",
      "done:  1050000\n",
      "done:  1060000\n",
      "done:  1070000\n",
      "done:  1080000\n",
      "done:  1090000\n",
      "done:  1100000\n",
      "done:  1110000\n",
      "done:  1120000\n",
      "done:  1130000\n",
      "done:  1140000\n",
      "done:  1150000\n",
      "done:  1160000\n",
      "done:  1170000\n",
      "done:  1180000\n",
      "done:  1190000\n"
     ]
    }
   ],
   "source": [
    "f = open(\"glove_from_stanford/glove.twitter.27B.200d.txt\", \"r\")\n",
    "words = []\n",
    "\n",
    "i = 0\n",
    "#embeddings = np.zeros((1,200))\n",
    "embeddings = []\n",
    "for l in f.readlines():\n",
    "    li = l.split()\n",
    "    w = li[0]\n",
    "    vec_string = li[1:]\n",
    "    vec = []\n",
    "    for e in vec_string:\n",
    "        vec.append(float(e))\n",
    "    vec = np.array(vec)\n",
    "    if i%10000 == 0:\n",
    "        print(\"done: \", i )\n",
    "    if vec.shape[0] == 200:\n",
    "        words.append(w)\n",
    "        #embeddings = np.vstack((embeddings, vec))\n",
    "        embeddings.append(vec)\n",
    "\n",
    "    else:\n",
    "        print(w, \" was not the right shape. The shape was: \", vec.shape)\n",
    "    i += 1\n",
    "    \n",
    "#embeddings = embeddings[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_stacked = np.stack(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embedding_stanford\", embedding_stacked)\n",
    "np.save(\"stanford_words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute vectors for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load(\"embedding_stanford.npy\")\n",
    "word_list = np.load(\"stanford_words.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = embedding.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin means\n"
     ]
    }
   ],
   "source": [
    "tweets_pos_txt = []\n",
    "f = open(\"Datasets/twitter-datasets/train_pos.txt\")\n",
    "for l in f.readlines():\n",
    "    tweets_pos_txt.append(l.strip())\n",
    "tweets_pos_txt = np.array(tweets_pos_txt)\n",
    "print(\"begin means\")\n",
    "tweets_vecs_pos = tweet_means(tweets_pos_txt, embedding, word_list, n_features, False, {}, False, False, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_neg_txt = []\n",
    "f = open(\"Datasets/twitter-datasets/train_neg.txt\")\n",
    "for l in f.readlines():\n",
    "    tweets_neg_txt.append(l.strip())\n",
    "tweets_neg_txt = np.array(tweets_neg_txt)\n",
    "tweets_vecs_neg = tweet_means(tweets_neg_txt, embedding, word_list, n_features, False, {}, False, False, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"tweets_X_sam.npy\")\n",
    "y = np.load(\"tweets_y_sam.npy\")\n",
    "\n",
    "j = 0.8\n",
    "\n",
    "train_X = X[0:int(j*len(y))]\n",
    "train_y = y[0:int(j*len(y))]\n",
    "test_X = X[int(j*len(y)):]\n",
    "test_y = y[int(j*len(y)):]\n",
    "\n",
    "train_y[train_y == -1.] = 0.\n",
    "test_y[test_y == -1.] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(250, activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='softmax')\n",
    "    #tf.keras.layers.Dense(50, activation='relu'),\n",
    "    #tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/15\n",
      "2500/2500 [==============================] - 10s 4ms/step - loss: 0.6495 - sparse_categorical_accuracy: 0.6164\n",
      "Epoch 2/15\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.5826 - sparse_categorical_accuracy: 0.6760\n",
      "Epoch 3/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5495 - sparse_categorical_accuracy: 0.7039\n",
      "Epoch 4/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5265 - sparse_categorical_accuracy: 0.7218\n",
      "Epoch 5/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5070 - sparse_categorical_accuracy: 0.7370\n",
      "Epoch 6/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4898 - sparse_categorical_accuracy: 0.7485\n",
      "Epoch 7/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4743 - sparse_categorical_accuracy: 0.7588\n",
      "Epoch 8/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4602 - sparse_categorical_accuracy: 0.7689\n",
      "Epoch 9/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4474 - sparse_categorical_accuracy: 0.7759\n",
      "Epoch 10/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4353 - sparse_categorical_accuracy: 0.7844\n",
      "Epoch 11/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4246 - sparse_categorical_accuracy: 0.7907\n",
      "Epoch 12/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4152 - sparse_categorical_accuracy: 0.7963\n",
      "Epoch 13/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.4055 - sparse_categorical_accuracy: 0.8018\n",
      "Epoch 14/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.3972 - sparse_categorical_accuracy: 0.8065\n",
      "Epoch 15/15\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.3884 - sparse_categorical_accuracy: 0.8119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64ebd3828>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.8010 - sparse_categorical_accuracy: 0.6836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8010182406425476, 0.6836]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open(\"train_supervised.txt\", \"w\")\n",
    "fout2 = open(\"test_supervised.txt\", \"w\")\n",
    "f1 = open(\"Datasets/twitter-datasets/train_pos_clean.txt\", \"r\")\n",
    "f2 = open(\"Datasets/twitter-datasets/train_neg_clean.txt\", \"r\")\n",
    "\n",
    "i = 0\n",
    "for l in f1.readlines():\n",
    "    fout.write(\"__label__positive \" + l)\n",
    "    if i > 80000:\n",
    "        fout2.write(\"__label__positive \" + l)\n",
    "    i += 1\n",
    "i = 0\n",
    "for l in f2.readlines():\n",
    "    fout.write(\"__label__negative \" + l)\n",
    "    if i > 80000:\n",
    "        fout2.write(\"__label__positive \" + l)\n",
    "    i+=1\n",
    "f1.close()\n",
    "f2.close()\n",
    "fout.close()\n",
    "fout2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ft.train_supervised('train_supervised.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__negative', '__label__positive']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.6640566]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21319, 0.5366574417186547, 0.5366574417186547)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"test_supervised.txt\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02571399,  0.15396671,  0.04118297, -0.21249573,  0.10300473,\n",
       "        0.0679355 , -0.09303714,  0.02835391,  0.16474135,  0.16493794,\n",
       "       -0.02768764, -0.07995272, -0.09622344,  0.09716307, -0.09126273,\n",
       "       -0.06234534,  0.08886703,  0.18623899, -0.15840052, -0.04535728,\n",
       "       -0.09760258,  0.15655494,  0.04274376,  0.0777818 , -0.04115377,\n",
       "       -0.20150737, -0.09738524, -0.11599219, -0.17654024,  0.15988488,\n",
       "       -0.17184366,  0.10775842, -0.09421516, -0.13489622, -0.27430215,\n",
       "       -0.08880118, -0.08983865, -0.0888577 ,  0.06397012, -0.01490111,\n",
       "       -0.07386695,  0.31255385,  0.06257832,  0.11577053,  0.13096482,\n",
       "       -0.05254146,  0.12596858, -0.10450681, -0.10654113,  0.24910197,\n",
       "       -0.12927958,  0.18731278, -0.11526599,  0.14024238, -0.14922841,\n",
       "        0.03247508, -0.18863636, -0.09864707,  0.10644189, -0.02615612,\n",
       "        0.00728449,  0.10776213,  0.15437657,  0.13160653, -0.01349203,\n",
       "       -0.09614967, -0.20270133, -0.02062804,  0.00064062, -0.10408647,\n",
       "       -0.17305616,  0.05169189, -0.00253879, -0.11106369, -0.07887001,\n",
       "       -0.02449477,  0.10974195,  0.03889515,  0.07962251, -0.01062912,\n",
       "       -0.0471232 ,  0.01600969,  0.10940198, -0.00197577,  0.08182845,\n",
       "       -0.08995222,  0.15674609, -0.04327465, -0.01629959, -0.07260174,\n",
       "        0.19479792,  0.00125491,  0.207558  , -0.02646345,  0.1441111 ,\n",
       "        0.01169929,  0.00484121,  0.13533713, -0.05212374, -0.02553489],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
