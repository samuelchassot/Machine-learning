\documentclass[11pt, a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{bookmark}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
 \geometry{
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm
 }

\graphicspath{ {./plots/} }

\begin{document}
\date{2019 December}
\title{CS-443 Machine Learning Project 2: TwitterOnFire}
\author{
  Julie Camille Rosalie Giunta\\
  \texttt{274957}
  \and
  Samuel Chassot\\
  \texttt{270955}
  \and
  Daniel Filipe Nunes Silva\\
  \texttt{275197}
}

\maketitle
\clearpage

\section{Introduction}
In this project, we implement tweets classifiers using neural nets, BERT, logistic regression and support-vector machine to predict if their content is positive or negative.

\section{Neural Networks}

\subsection{Preprocessing}
To clean our data, we follow an algorithm similar to Kim Yoon's paper\cite{kimyoonpaper} and code\cite{kimyooncode}. We remove duplicates on the training set and separate known contractions. For example, "can'nt" becomes "can 'nt".

\subsection{Embedding and Neural Nets}
The first part is to transform each tweets in a vector. We first try using an implementation of the 
GloVE algorithm of our own, the results are not very satisfiing (65\%-70\%). We then use an embedding 
created by the Stanford University which is also based on GloVE and that used a big Twitter dataset\cite{glovepaper, gloveembedding}. 
We decide to use the 200 features version.
We then compute the vector for each tweets as the numerical average of each vector corresponding to the 
words of which it is composed.

To do the Neural Nets and the training, we use Tensorflow library from Google and run it on a Google Colab notebook 
to have access to the GPUs they offer.

We try lots of NN configuration (at random but manually chosen). The one that give best results are the one with 
few hidden layers but lots of neuron per layer. We also add in some of them some Dropout layers with $p\in [0.05, 0.2]$ 
depending of the size of the layer before.
the best achieve 84.3\% on AIcrowd has one input layer of $200$, one hidden layer of $100,000$, one dropout 
layer with $p=0.15$ and an output layer of 2 nodes. All hidden layers use the ReLU function, we try others 
but that give poor results. The output layer uses softmax activation function.

We use standard accuracy as loss function. The optimizer used is first 'Adam' which we replace by SGD for better results.

We try to train the NN on TPUs provided by Google Colab but as everything is in beta, we cannot make it work.

The number of epochs can only be approximate due all the crashes encountered on Colab but lives around 150 per model.

Considering the difficulties encountered during training process (due mainly to the computational cost), we try lots of NN 
structures and keep only interesting ones (that beat previous best score on AIcrowd). 
Here a list of structures and their corresponding score on AIcrowd. Only the hidden layers are shown.
\begin{itemize}
  \item \textbf{$81.8\%$}: $\rightarrow 25000\rightarrow 1000\rightarrow 500\rightarrow $
  \item \textbf{$84.2\%$}: $\rightarrow 1000\rightarrow \text{Drop}\rightarrow 1000
  \rightarrow \text{Drop}\rightarrow 1000\rightarrow \text{Drop}\rightarrow 1000
  \rightarrow \text{Drop}\rightarrow 1000\rightarrow \text{Drop}\rightarrow $ where Drop is a Dropout layer with $p=0.1$.
  \item \textbf{$84.3\%$}: $\rightarrow 100000\rightarrow \text{Drop}\rightarrow $ where Drop is a Dropout layer with $p=0.15$
\end{itemize}

\subsection{Convolutional Neural Networks}
We also use Stanford University GloVE's embedding\cite{glovepaper, gloveembedding}. To reduce the computational complexity, we reduce the embedding matrix to the most used word vectors and the tweets to contain only words that are in the embedding matrix. We detect that the maximum number of useful tokens in our data set is 199\label{199}.

We compare 3 methods:


\section{BERT}
BERT (Bidirectional Encoder Representations from Transformers)\cite{bertpaper} is a training process made by researchers at Google, that, instead of reading a sentence in a certain order, uses the entire sequence simultaneously.

We use ktrain library\cite{ktrain} and inspire ourself from its tutorial on text classification to try BERT on our data. We remove duplicates from our training set and then split it in $75\%$ for training and $25\%$ for validating. To reduce the computational complexity and the use of RAM, we fix the batch size to 10 and the maximal number of tokens to consider to 199, as explained in \ref{199}. As adviced in the original paper, we use a learning rate of $2*10^{-5}$. We do only one epoch with the fit\_onecycle method since it is time consumming. This technique corresponds to our best submission on AICrowd with an accuracy of $87.6\%$ and an F-1 score of $87.9\%$.

We then try to increase the maximal number of tokens to 400, as it is in reality in our dataset but it decreased our score on AICrowd of $0.01\&$. We also try with 3 epochs, a maximal number of tokens of 128 and a batch size of 32 (to make it scalable) but our result in AICrowd is then decreased of $1.0\%$.

\section{Logistic Regression and Support-Vector Machine}
In this section we take a different approach compared to the one of neural nets. First, we present the preprocessing steps we applied to clean, normalize and transforme the provided tweets datasets. Then, we compare them by training two models using support-vector machine and logistic regression classifiers. Finally, we optimize the most accurate models.

We used the following libraries and their dependencies.
\begin{itemize}
	\setlength\itemsep{1px}
	\item Python 3.8
	\item Jupyter-Notebook
	\item Natural Language Toolkit
	\item Scikit-learn
\end{itemize}

\subsection{Preprocessing}
We applied three kinds of preprocessing steps.
\begin{itemize}
	\setlength\itemsep{1px}
	\item Removing or replacing words and symbols.
	\item Normalizing by modifying words or sentences.
	\item Transforming the tweets into usable data structures for machine learning.
\end{itemize}

\subsubsection{Tweet Cleaning}
This is a basic operation. We get rid of \texttt{<User>}, \texttt{<url>} and \verb"\n" because these have already been processed and should not contain that much information. Notice that nice users bad users are all described by the same word. The same argument can be applied to URLs. We also use regular expression to remove all kinds of punctuation. Even if punctation is usually expressive, it may be inconsistent considering the variety of tweets.

\subsubsection{Duplicates}
We simply remove all the duplicates tweets in the cleaned datasets. The problem with duplicates is that they can artificially increase the accuracy if the sames tweets appear in the training as well as in the test dataset. Nevertheless, removing duplicates do not necessarly make sense as multiple user could have written the same tweet. We would neglect its importance.

\subsubsection{Stemming}
Stemming is a technique that removes affixes so that similar words are effectively understood as similar when interpreted by the machine learning algorithm. Here are some examples : \textit{cars - car, swimming - swimm, this - thi}. This is often described as brute force as it is not smart but crude. For example, it does not see the difference between a word in its plural form from a word whose last letter is an \textit{s}.

\subsubsection{Lemmatization}
This technique is the smart version of stemming. It does a more advanced analysis of word to clearly identify the structure of each word and to cut it correctly.

\subsubsection{Stop Words}
Stop words are the most common words like \textit{I, it, is, of, in, \dots}. They should be equivalently be present in both positive and negative datasets but they do not have real impact in the meaning of the sentence. We went trough all the datasets to establish a list of the most used words, kept the ten most used and removed them in all the tweets of the training sets.

\subsubsection{Vectorization}
We use Scikit-learn to vectorize our data. We used multiple binary vectorization with \textit{n-grams} where \textit{n} corresponds to the number of words vectorized together. This helps considering the context in which words are used by increasing \textit{n}.

\subsubsection{Term Frequency-Inverse Document Frequency}
Instead of using \textit{0s} and \textit{1s} like binary vectorization, with \textit{TF-IDF} we count the occurences of the words.

\subsection{1-grams Logistic Regression and Support-Vector Machine}
First, we select the most effective cleaning or normalizing method by assessing them with logistic regression and SVM on a 1-grams vectorization. We use 75\% of the provided datasets for the training and the 25\% remaining for testing.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../plots/logreg.png}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{../plots/svm.png}
\end{figure}

Overall, SVM performs better than logistic regression. Simple cleaning seems to be enough. Since tweets are short sentences, it is hard to normalize tweets without alterating them too much.

\subsection{Optimization of SVM}
Finally, we optimize the SVM model on cleaned data.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{../plots/improved_svm.png}
\end{figure}

By taking 3 and 4-grams vectorization, we are able to improve from 82\% to 86\% even if 4-grams seems to be the limit before sever overfitting. Nevertheless, this score is confirmed on aicrowd. On the other hand, \textit{TF-IDF} performs similarly to 1-grams as tweets are short and words tend to appear only once in them.

\section{Final Discussion}
\begin{itemize}
	\item Parler de la difficulté d'extraire de l'info des tweets car ils sont court
	\item Parler que les limites de logreg et svm semblement atteintes
	\item Parler grossièrement du temps de calcul
	\item Parler de LSTM et RNN
\end{itemize}

\clearpage
\onecolumn

\begin{thebibliography}{9}
	\bibitem{kimyoonpaper}
		KIM, Yoon. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.
	\bibitem{kimyooncode}
		CNNs for sentence classification
		\\\url{https://github.com/yoonkim/CNN\_sentence}
	\bibitem{glovepaper}
		PENNINGTON, Jeffrey, SOCHER, Richard, et MANNING, Christopher. Glove: Global vectors for word representation. In : Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014. p. 1532-1543.
	\bibitem{gloveembedding}
		GloVE Twitter pre-trained word vectors
		\\\url{https://nlp.stanford.edu/projects/glove}
	\bibitem{bertpaper}
		DEVLIN, Jacob, CHANG, Ming-Wei, LEE, Kenton, et al. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
	\bibitem{bertcode}
		BERT Text Classification in 3 Lines of Code Using Keras
		\\\url{https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358}
	\bibitem{ktrain}
		Ktrain library
		\\\url{https://github.com/amaiya/ktrain}
	\bibitem{sentianal}
		Sentiment Analysis with Python
		\\\url{https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184}
		\\\url{https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a}
	\bibitem{stemlema}	
		Stemming and Lemmatization
		\\\url{https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html}

\end{thebibliography}

\end{document}
