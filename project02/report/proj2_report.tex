\documentclass[11pt, a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
 \geometry{
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm
 }

\graphicspath{ {./plots/} }

\begin{document}
\date{2019 December}
\title{CS-443 Machine Learning Project 2: TwitterOnFire}
\author{
  Julie Camille Rosalie Giunta\\
  \texttt{274957}
  \and
  Samuel Chassot\\
  \texttt{270955}
  \and
  Daniel Filipe Nunes Silva\\
  \texttt{275197}
}

\maketitle
\clearpage

\section{Introduction}
The goal of the project was to classify a set of tweets as positive or negative content. We try to use different methods to achieve this goal.

\begin{itemize}
  \item Neural Nets
	\item Logistic Regression and Support-Vector Machine
\end{itemize}

\section{Embedding and Neural Nets}
The first part is to transform each tweets in a vector. We first try using an implementation of the GloVE algorithm of our own, the results are not very satisfiing (65\%-70\%). We then use an embedding created by the Stanford University which is also based on GloVE and that used a big Twitter dataset. We decide to use the 200 features version.
We then compute the vector for each tweets as the numerical average of each vector corresponding to the words of which it is composed.

To do the Neural Nets and the training, we use Tensorflow library from Google and run it on a Google Colab notebook to have access to the GPUs they offer.

We try lots of NN configuration (at random but manually chosen). The one that give best results are the one with few hidden layers but lots of neuron per layer. We also add in some of them some Dropout layers with $p\in [0.05, 0.2]$ depending of the size of the layer before.
the best achieve 84.3\% on Aiwrowd has one input layer of $200$, one hidden layer of $100,000$, one dropout layer with $p=0.15$ and an output layer of 2 nodes. All hidden layers use the ReLU function, we try others but that give poor results. The output layer uses softmax activation function.

We use standard accuracy as loss function. The optimizer used is first 'Adam' which we replace by SGD for better results.

\section{Logistic Regression and Support-Vector Machine}
In this section we take a different approach compared to the last one about neural nets. First, we present the preprocessing steps we applied to clean, normalize and transforme the provided tweets datasets. Then, we compare them by training two models using support-vector machine and logistic regression classifier. Finally, we optimize the most accurate models.

\subsection{Preprocessing}
We applied three differents kinds of preprocessing steps.
\begin{itemize}
	\item Removing or replacing words and symbols.
	\item Normalizing by modifying words or sentences.
	\item Transforming the tweets into usable data structures for machine learning.
\end{itemize}

\subsubsection{Tweet Cleaning}
\subsubsection{Duplicate}
\subsubsection{Stemming}
\subsubsection{Lemmatization}
\subsubsection{Stop Words}
\subsubsection{Stop Words}
\subsubsection{Vectorization}
\subsubsection{Term Frequency-Inverse Document Frequency}

\subsection{Logistic Regression}

\subsection{SVM}

\subsection{Optimization of SVM}

\end{document}

