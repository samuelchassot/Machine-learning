{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"sam_tensorflow_bis_model-1.ipynb","provenance":[],"collapsed_sections":["AWlgkBnq0nqY","vRKdN5K_sTZH","7wi8vdJX5ixu","edPmrZmLsTZU","FllJKRpo3gwK","8GqoxXIPUjsY","7TErR5mKyXAD","AiHbrf1TM3hn"],"toc_visible":true,"machine_shape":"hm"},"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Introduction\n","\n","This notebook is the notebook used for neural nets training on Google Colab. To run it, you will need to install tensorflow. Moreover, loading the stanford embedding takes around 6 hours and computing the vectors for the full dataset takes around 10 hours. This is why there some savings of numpy arrays as checkpoints."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AWlgkBnq0nqY"},"outputs":[],"source":["# Helpers"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["import numpy as np\n","import os\n","from pattern import *\n","\n","#Spelling changes\n","spelling_dict = {\"u\" : \"you\", \"dont\" : \"don't\", \"cant\" : \"can't\", \"r\" : \"are\", \"wont\" : \"won't\"}\n","\n","#Common words to remove\n","common = ['\"', ',', '.', ')', '(', '-', \\\n","          \"<url>\", \"a\", \"the\", \"of\", \"to\", \\\n","          \"it\", \"this\", \"that\", \"these\", \"there\"]\n","\n","def words_list(file_name):\n","    words_list = []\n","    f = open(file_name, \"r\")\n","    for l in f.readlines():\n","        l = l.strip()\n","        words_list.append(l)\n","    words_list = np.array(words_list)\n","    f.close()\n","    return words_list\n","\n","def tweets_txt(file_name):\n","    tweets_txt = []\n","    f = open(file_name, \"r\")\n","    for l in f.readlines():\n","        tweets_txt.append(l.strip())\n","    f.close()\n","    return np.array(tweets_txt)\n","\n","def tweet_means(tweets_txt, word_embeddings, words_list, embedding_size, \\\n","                spelling=False, spelling_dict=dict(), \\\n","                negation=False, \\\n","                clean=False, common=[]):\n","    tweets_vec = []\n","    i = 0\n","    for tw in tweets_txt:\n","        words_in_tweet = tw.split(\" \")\n","        if(spelling):\n","            words_in_tweet = transform_spelling(words_in_tweet, spelling_dict)\n","        if(negation):\n","            words_in_tweet = transform_negation(words_in_tweet)\n","        if(clean):\n","            words_in_tweet = remove_words(words_in_tweet, common)\n","            words_in_tweet = remove_exclamation(words_in_tweet)\n","        acc = np.zeros(embedding_size)\n","        for w in words_in_tweet:\n","            vec = word_embeddings[np.argmax(words_list==w)]\n","            acc += vec\n","        acc = acc/len(words_in_tweet)\n","        tweets_vec.append(acc)\n","        if i%1000 == 0:\n","            print(i, \" done\")\n","        i += 1\n","    tweets_vec = np.array(tweets_vec)\n","    return tweets_vec\n","\n","def remove_duplicated_tweets_txt(tweets):\n","    return np.unique(tweets, axis = 0)\n","\n","def remove_duplicated_tweets(X, y):\n","    tmp = np.hstack((X, y.reshape((len(y),1))))\n","    tmp = np.unique(tmp, axis=0)\n","\n","    new_X = tmp[:,:-1]\n","    new_y= tmp[:,-1]\n","    return new_X, new_y\n","\n","def remove_words(words, tweet):\n","    \"\"\" Remove the words that are in the list <words> from the tweets \"\"\"\n","    filtered_tweet = [w for w in tweet if w not in words]   \n","    return filtered_tweet\n","\n","def remove_exclamation(tweet):\n","    \"\"\" Remove the \"!!!\" that may be at the beginning of tweets \"\"\"\n","    if(tweet[0:3] == ['!', '!', '!']):\n","        return tweet[3:]\n","    return tweet\n","\n","def transform_negation(tweet):\n","    \"\"\"Transform a negated verb into an infinitive form + not\n","        ex: don't -> do not\n","    \"\"\"\n","    new_tweet = []\n","    for w in tweet:\n","        #We check if the verb is negated and if the pattern library knows its infinitive form\n","        if(\"n't\" in w and conjugate(w) != w):\n","            new_tweet.append(conjugate(w))\n","            new_tweet.append(\"not\")\n","        else:\n","            new_tweet.append(w)\n","    \n","    return new_tweet\n","\n","def transform_spelling(tweet, spelling_dict):\n","    \"\"\" Replace the words of a tweet by another spelling if they are in <spelling_dict> \"\"\"\n","    new_tweet = [spelling_dict.get(w, w) for w in tweet]\n","    return new_tweet"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5tv4Bw-T1ExI"},"outputs":[],"source":["# Imports for Colab"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["import tensorflow as tf\n","%tensorflow_version 2.x\n","import numpy as np\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('No GPU Found')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vRKdN5K_sTZH"},"outputs":[],"source":["# Load GloVe embedding from Stanford"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["f = open(\"glove_from_stanford/glove.twitter.27B.200d.txt\", \"r\")\n","words = []\n","\n","i = 0\n","embeddings = []\n","for l in f.readlines():\n","    li = l.split()\n","    w = li[0]\n","    vec_string = li[1:]\n","    vec = []\n","    for e in vec_string:\n","        vec.append(float(e))\n","    vec = np.array(vec)\n","    if i%10000 == 0:\n","        print(\"done: \", i )\n","    if vec.shape[0] == 200:\n","        words.append(w)\n","        embeddings.append(vec)\n","\n","    else:\n","        print(w, \" was not the right shape. The shape was: \", vec.shape)\n","    i += 1"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"0XGaB9g5sTZM"},"outputs":[],"source":["embedding_stacked = np.stack(embeddings, axis=0)"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"LKkVyxzgsTZO"},"outputs":[],"source":["words = np.array(words)"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"3RjLVbzesTZR"},"outputs":[],"source":["np.save(\"embedding_stanford.npy\", embedding_stacked)\n","np.save(\"stanford_words.npy\", words)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7wi8vdJX5ixu"},"outputs":[],"source":["# Reduce stanford embedding for this particular dataset"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"Q-pTrgD15oXg"},"outputs":[],"source":["embedding_stanford = np.load(\"embedding_stanford.npy\")\n","word_list_stanford = np.load(\"words_stanford.npy\")\n","\n","words_list_full_dataset = np.load(\"words_full_list_clean_kim_yoon.npy\")\n"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"Lp6Ha0HQ56qL"},"outputs":[],"source":["words_needed = np.isin(word_list_stanford, words_list_full_dataset)"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"TxmFeYTK6pkm"},"outputs":[],"source":["word_wanted_indices = np.nonzero(words_needed*1.)[0]"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"25j6MqxQ8NOk"},"outputs":[],"source":["reduced_embedding_stanford = embedding_stanford[word_wanted_indices]\n","reduced_words_stanford = word_list_stanford[word_wanted_indices]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FllJKRpo3gwK"},"outputs":[],"source":["# Compute vectors for tweets FULL dataset"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"GB1CUa9x4gCw"},"outputs":[],"source":["embedding = reduced_embedding_stanford\n","word_list = reduced_words_stanford\n","n_features = embedding.shape[1]"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["tweets_pos_full_txt = []\n","f = open(\"data/train_pos_full_clean.txt\")\n","for l in f.readlines():\n","  tweets_pos_full_txt.append(l.strip())\n","tweets_pos_full_txt = np.array(tweets_pos_full_txt)\n","tweets_vecs_pos_full = tweet_means(tweets_pos_full_txt, embedding, word_list, n_features, False, {}, False, False, [])\n","np.save(\"tweets_pos_full_clean_stanford.npy\", tweets_vecs_pos_full)"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["tweets_neg_full_txt = []\n","f = open(\"data/train_neg_full_clean.txt\")\n","for l in f.readlines():\n","    tweets_neg_full_txt.append(l.strip())\n","tweets_neg_full_txt = np.array(tweets_neg_full_txt)\n","tweets_vecs_neg_full = tweet_means(tweets_neg_full_txt, embedding, word_list, n_features, False, {}, False, False, [])\n","np.save(\"tweets_neg_full_clean_stanford.npy\", tweets_vecs_neg_full)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GyxGw7rCCfKg"},"outputs":[],"source":["\n","# Load saved data FULL (FROM HERE to test models)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":25628,"status":"ok","timestamp":1576481560073,"user":{"displayName":"Samuel Chassot","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64","userId":"17892117135253630625"},"user_tz":-60},"id":"TZtasSvK4nR6","outputId":"11ef569c-9258-4c6e-aff0-b0c3e368d022"},"outputs":[],"source":["\n","tweets_pos = np.load(\"tweets_pos_full_clean_stanford.npy\")\n","tweets_neg = np.load(\"tweets_neg_full_clean_stanford.npy\")\n","X = np.vstack((tweets_pos, tweets_neg))\n","y = np.array([1 for i in range(tweets_pos.shape[0])] + [-1 for i in range(tweets_neg.shape[0])])\n","\n","indices = np.random.permutation([i for i in range(y.shape[0])])\n","\n","X = X[indices]\n","y = y[indices]\n","\n","print(X.shape)\n","print(y.shape)\n","\n","#because of protobuf limit of 2GB\n","X = X.astype(np.float32)\n","y = y.astype(np.float32)\n","\n","n_features=X.shape[1]\n","\n","j = 0.9\n","\n","train_X = X[0:int(j*len(y))]\n","train_y = y[0:int(j*len(y))]\n","test_X = X[int(j*len(y)):]\n","test_y = y[int(j*len(y)):]\n","\n","train_y[train_y == -1.] = 0.\n","test_y[test_y == -1.] = 0.\n","\n","#free some ram\n","tweets_pos = 0.0\n","tweets_neg = 0.0"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RrSCx74MCoyc"},"outputs":[],"source":["# Work with data GPU"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"HgH_Ed-PsTZh"},"outputs":[],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.InputLayer(n_features))\n","model.add(tf.keras.layers.Dense(1000, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(1000, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(1000, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(1000, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.1))\n","model.add(tf.keras.layers.Dense(500, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.05))\n","model.add(tf.keras.layers.Dense(2, activation='softmax'))\n","model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                metrics=['accuracy'])\n","model.save('models/model_temp.h5')"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"Vaf2wYnCsTZf"},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n","\n","BATCH_SIZE = 32\n","SHUFFLE_BUFFER_SIZE = 100\n","\n","train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","test_dataset = test_dataset.batch(BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ixDEqZvnJcCk"},"outputs":[],"source":["Here I often save the model and reload it because the Colab notebook experienced some crash so with this technique, we do not loose to much work."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","executionInfo":{"elapsed":6486,"status":"error","timestamp":1576577701803,"user":{"displayName":"Samuel Chassot","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64","userId":"17892117135253630625"},"user_tz":-60},"id":"-Hh7yxbxIQIB","outputId":"315617da-d6c5-4d4e-d6ee-3220497addd3"},"outputs":[],"source":["n_epochs = 5\n","for _ in range(15):\n","  model = tf.keras.models.load_model('models/model_temp-1.h5')\n","  model.fit(train_dataset, epochs=3)\n","  f = open(\"models/number-of-it-done-on-model-temp-1.txt\", 'a')\n","  f.write(str(n_epochs) + '\\n')\n","  f.close()\n","  model.save('/content/drive/My Drive/Colab Notebooks/ML-MA1/Project02/models/model_temp-1.h5')\n","  model.evaluate(test_dataset)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":14896,"status":"ok","timestamp":1576593289957,"user":{"displayName":"Samuel Chassot","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64","userId":"17892117135253630625"},"user_tz":-60},"id":"xK_qIneKKBI9","outputId":"72c0b469-3a06-4c90-ae6c-b21b9a12a230"},"outputs":[],"source":["model = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/ML-MA1/Project02/models/model_temp-1.h5')\n","model.evaluate(test_dataset)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AiHbrf1TM3hn"},"outputs":[],"source":["# Load test data and output for AIcrowd"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":117657,"status":"ok","timestamp":1576245199724,"user":{"displayName":"Samuel Chassot","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALD7pkhpuIA1g72SFrp9LN4wuGALa406781WPi=s64","userId":"17892117135253630625"},"user_tz":-60},"id":"gcnwqKa5sTZt","outputId":"a24d0fd2-1697-4cab-8c42-53d8684a0ad1"},"outputs":[],"source":["embedding = reduced_embedding_stanford\n","word_list = reduced_words_stanford\n","n_features = embedding.shape[1]\n","\n","tweets_test_txt = []\n","f = open(\"data/test_data_clean_kim_yoon.txt\")\n","for l in f.readlines():\n","    l = l.strip()\n","    l = l[l.find(',')+1:]\n","    tweets_test_txt.append(l.strip())\n","tweets_test_txt = np.array(tweets_test_txt)\n","print(\"begin means\")\n","tweets_vecs_test = tweet_means(tweets_test_txt, embedding, word_list, n_features, False, {}, False, False, [])\n","\n","np.save(\"tweets_test_clean_stanford.npy\", tweets_vecs_test)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mSpdx5BbgW08"},"outputs":[],"source":["# Make CSV"]},{"cell_type":"code","metadata":{"colab":{},"colab_type":"code","id":"3SR1QUfqbZxU"},"outputs":[],"source":["model = tf.keras.models.load_model('models/model_temp.h5')\n","tweets_vecs_test = np.load(\"tweets_test_clean_stanford.npy\")\n","y = model.predict(tweets_vecs_test)\n","y = np.argmax(y, axis=1)\n","\n","# make csv\n","with open(\"submission.csv\", \"w\") as f:\n","  f.write(\"Id,Prediction\\n\")\n","  id = 1\n","  for i in y:\n","    if i == 0:\n","      i = -1\n","    l = str(id) + \",\" + str(i) + \"\\n\"\n","    f.write(l)\n","    id = id + 1"]}]}