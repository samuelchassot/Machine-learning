{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN implementation for Tweets Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this notebook uses tensorflow and needs a lot of computation power, it was ran on Google Colab and we advice you to do the same.\n",
    "\n",
    "You can find the GloVE embedding here : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To use the following notebook, you need to have the following directory:\n",
    " \n",
    "Current Directory\n",
    " ├── Datasets\n",
    " │   ├── twitter-datasets\n",
    " │   │   ├── train_neg_full.txt       \n",
    " │   │   ├── train_pos_full.txt\n",
    " │   │   ├── test_data.txt\n",
    " |\n",
    " ├── glove_from_stanford\n",
    " |   ├── glove.twitter.27B.200d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  #raise SystemError('GPU device not found')\n",
    "  print('No GPU Found')\n",
    "    \n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split             \n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kim_yoon(filename, unique):\n",
    "    \"\"\" Clean txt files using a similar algorithm as kim yoon's one \n",
    "    (see https://github.com/yoonkim/CNN_sentence)\n",
    "    \n",
    "    Removes duplicates if unique is set to True.\n",
    "    Splits known contractions. Ex : do -> do n't\n",
    "    \"\"\"\n",
    "    new_filename = filename.replace(\".txt\", \"_clean_kim_yoon.txt\")\n",
    "\n",
    "    prev_f = open(filename, \"r\")\n",
    "    new_f = open(new_filename, \"w+\")\n",
    "\n",
    "    if(unique):\n",
    "        tweets = list(set(prev_f.readlines()))\n",
    "    else:\n",
    "        tweets = prev_f.readlines()\n",
    "\n",
    "    for t in tweets:\n",
    "        t1 = re.sub(r\"\\'s\", \" \\'s\", t) \n",
    "        t1 = re.sub(r\"\\'ve\", \" \\'ve\", t1) \n",
    "        t1 = re.sub(r\"n\\'t\", \" n\\'t\", t1) \n",
    "        t1 = re.sub(r\"\\'re\", \" \\'re\", t1) \n",
    "        t1 = re.sub(r\"\\'d\", \" \\'d\", t1) \n",
    "        t1 = re.sub(r\"\\'ll\", \" \\'ll\", t1)\n",
    "        t1 = re.sub(r\"\\'m\", \" \\'m\", t1)\n",
    "        new_f.write(t1)\n",
    "\n",
    "    prev_f.close()\n",
    "    new_f.close()\n",
    "    \n",
    "def tweets_txt(file_name):\n",
    "    \"\"\"Parse a file and return an array of tweets\"\"\"\n",
    "    tweets_txt = []\n",
    "    f = open(file_name, \"r\")\n",
    "    for l in f.readlines():\n",
    "        tweets_txt.append(l.strip())\n",
    "    f.close()\n",
    "    return np.array(tweets_txt)\n",
    "    \n",
    "def remove_not_in_words_list(tweets, words_list):\n",
    "    \"\"\"Remove from tweets all tokens that are not in words_list\"\"\"\n",
    "    reduced_tweets = []\n",
    "    for t in tweets:\n",
    "        t_words = t.split(\" \")\n",
    "        new_t = [w for w in t_words if np.any(words_list == w)]\n",
    "        reduced_tweets.append(\" \".join(new_t))\n",
    "    return reduced_tweets\n",
    "\n",
    "def max_tweet_length(tweets):\n",
    "    \"\"\"Compute the maximum number of tokens of a set of tweets\"\"\"\n",
    "    max_length = 0\n",
    "    for t in tweets:\n",
    "        max_length = max(max_length, len(t))\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_kim_yoon(\"Datasets/twitter-datasets/train_neg_full.txt\", True)\n",
    "clean_kim_yoon(\"Datasets/twitter-datasets/train_pos_full.txt\", True)\n",
    "clean_kim_yoon(\"Datasets/twitter-datasets/test_data.txt\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"glove_from_stanford/glove.twitter.27B.200d.txt\", \"r\")\n",
    "words = []\n",
    "\n",
    "i = 0\n",
    "embeddings = []\n",
    "for l in f.readlines():\n",
    "    li = l.split()\n",
    "    w = li[0]\n",
    "    vec_string = li[1:]\n",
    "    vec = []\n",
    "    for e in vec_string:\n",
    "        vec.append(float(e))\n",
    "    vec = np.array(vec)\n",
    "    if i%10000 == 0:\n",
    "        print(\"done: \", i )\n",
    "    if vec.shape[0] == 200:\n",
    "        words.append(w)\n",
    "        embeddings.append(vec)\n",
    "\n",
    "    else:\n",
    "        print(w, \" was not the right shape. The shape was: \", vec.shape)\n",
    "    i += 1\n",
    "    \n",
    "embedding_stacked = np.stack(embeddings, axis=0)\n",
    "words = np.array(words)\n",
    "\n",
    "np.save(\"embedding_stanford.npy\", embedding_stacked)\n",
    "np.save(\"words_stanford.npy\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this code, you should apply build_vocab.sh & cut_vocab.sh to the cleaned txt files that contain the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"vocab_cut_clean_kim_yoon.txt\", 'r')\n",
    "words = []\n",
    "for l in f.readlines():\n",
    "    words.append(l[:-1])\n",
    "words = np.array(words)\n",
    "f.close()\n",
    "np.save(\"words_full_list_clean_kim_yoon.npy\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_stanford = np.load(\"embedding_stanford.npy\")\n",
    "word_list_stanford = np.load(\"words_stanford.npy\")\n",
    "\n",
    "words_list_full_dataset = np.load(\"words_full_list_clean_kim_yoon.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_needed = np.isin(word_list_stanford, words_list_full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_wanted_indices = np.nonzero(words_needed*1.)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embedding_kim_yoon = embedding_stanford[word_wanted_indices]\n",
    "reduced_words_kim_yoon = word_list_stanford[word_wanted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"reduced_embedding_kim_yoon.npy\", reduced_embedding_kim_yoon)\n",
    "np.save(\"reduced_words_kim_yoon.npy\", reduced_words_kim_yoon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce tweets depending on new embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = tweets_txt(\"Datasets/twitter-datasets/train_pos_full_clean_kim_yoon.txt\")\n",
    "tweets_neg = tweets_txt(\"Datasets/twitter-datasets/train_neg_full_clean_kim_yoon.txt\")\n",
    "\n",
    "tweets_test= tweets_txt(\"Datasets/twitter-datasets/test_data_clean_kim_yoon.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_reduced = remove_not_in_words_list(tweets_pos, reduced_words_kim_yoon)\n",
    "np.save(\"Datasets/twitter-datasets/reduced_full_tweets_pos.npy\", tweets_pos_reduced)\n",
    "tweets_neg_reduced = remove_not_in_words_list(tweets_neg, reduced_words_kim_yoon)\n",
    "np.save(\"Datasets/twitter-datasets/reduced_full_tweets_neg.npy\", tweets_neg_reduced)\n",
    "tweets_test_reduced = remove_not_in_words_list(tweets_test, reduced_words_kim_yoon)\n",
    "np.save(\"Datasets/twitter-datasets/reduced_tweets_test.npy\", tweets_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for every method except method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = np.load(\"Datasets/twitter-datasets/reduced_full_tweets_pos.npy\")\n",
    "tweets_neg = np.load(\"Datasets/twitter-datasets/reduced_full_tweets_neg.npy\")\n",
    "\n",
    "max_l1 = max_tweet_length(tweets_pos)\n",
    "max_l2 = max_tweet_length(tweets_neg)\n",
    "\n",
    "maxlen = max(max_l1, max_l2)\n",
    "\n",
    "tweets = np.concatenate((tweets_pos, tweets_neg))\n",
    "y = np.concatenate((np.ones((tweets_pos.shape[0])), np.zeros((tweets_neg.shape[0]))))\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(\n",
    "                                                tweets, y,  \n",
    "                                                test_size=0.25,  \n",
    "                                                random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=reduced_words_kim_yoon.shape[0])\n",
    "tokenizer.fit_on_texts(tweets_train)\n",
    "\n",
    "X_train_full = tokenizer.texts_to_sequences(tweets_train)\n",
    "X_test_full = tokenizer.texts_to_sequences(tweets_test)\n",
    "\n",
    "X_train_full = pad_sequences(X_train_full, padding='post', maxlen=maxlen)\n",
    "X_test_full = pad_sequences(X_test_full, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos = np.load(\"Datasets/twitter-datasets/reduced_full_tweets_pos.npy\")\n",
    "tweets_neg = np.load(\"Datasets/twitter-datasets/reduced_full_tweets_neg.npy\")\n",
    "\n",
    "tweets = np.concatenate((tweets_pos, tweets_neg))\n",
    "positive_labels = [[0, 1] for _ in tweets_pos]\n",
    "negative_labels = [[1, 0] for _ in tweets_neg]\n",
    "y_2 = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(\n",
    "                                                tweets, y_2,  \n",
    "                                                test_size=0.25,  \n",
    "                                                random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=reduced_words_kim_yoon.shape[0])\n",
    "tokenizer.fit_on_texts(tweets_train)\n",
    "\n",
    "X_train_full = tokenizer.texts_to_sequences(tweets_train)\n",
    "X_test_full = tokenizer.texts_to_sequences(tweets_test)                        \n",
    "\n",
    "max_l1 = max_tweet_length(tweets_pos)\n",
    "max_l2 = max_tweet_length(tweets_neg)\n",
    "\n",
    "maxlen = max(max_l1, max_l2)\n",
    "\n",
    "X_train_full = pad_sequences(X_train_full, padding='post', maxlen=maxlen)\n",
    "X_test_full = pad_sequences(X_test_full, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "Adapted from : https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_full, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test_full, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and submission for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = np.load(\"Datasets/twitter-datasets/reduced_tweets_test.npy\")\n",
    "tweets_test = tokenizer.texts_to_sequences(tweets_test)\n",
    "tweets_test = pad_sequences(tweets_test, padding='post', maxlen=maxlen)\n",
    "y = model.predict_classes(tweets_test)\n",
    "\n",
    "# make csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    id = 1\n",
    "    for i in y:\n",
    "        if i == 0:\n",
    "            i = -1\n",
    "        if i == 1:\n",
    "            i = 1\n",
    "        l = str(id) + \",\" + str(i) + \"\\n\"\n",
    "        f.write(l)\n",
    "        id = id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "Adapted from : https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/data_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = reduced_embedding_kim_yoon.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding = layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            embedding_dim,\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen)(inputs)\n",
    "reshape = layers.Reshape((maxlen,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = layers.Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = layers.Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = layers.Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = layers.MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = layers.MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = layers.MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = layers.Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = layers.Flatten()(concatenated_tensor)\n",
    "dropout = layers.Dropout(drop)(flatten)\n",
    "output = layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_full, y_train,\n",
    "                    epochs=10, callbacks=[checkpoint],\n",
    "                    validation_data=(X_test_full, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and submission for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = np.load(\"Datasets/twitter-datasets/reduced_tweets_test.npy\")\n",
    "tweets_test = tokenizer.texts_to_sequences(tweets_test)\n",
    "tweets_test = pad_sequences(tweets_test, padding='post', maxlen=maxlen)\n",
    "y = model.predict_classes(tweets_test)\n",
    "\n",
    "# make csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    id = 1\n",
    "    for i in y:\n",
    "        if i == 0:\n",
    "            i = -1\n",
    "        if i == 1:\n",
    "            i = 1\n",
    "        l = str(id) + \",\" + str(i) + \"\\n\"\n",
    "        f.write(l)\n",
    "        id = id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "Adapted from : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without dropouts\n",
    "Best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with one dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with two dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 3 dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with a dense layer of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with filters=256 in the first Conv1D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(256, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with filters=256 in the first Conv1D layer and 1000 in the first dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(256, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with filters=512 in the first Conv1D layer and 1000 in the first dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(reduced_embedding_kim_yoon.shape[0],\n",
    "                            reduced_embedding_kim_yoon.shape[1],\n",
    "                            weights=[reduced_embedding_kim_yoon],\n",
    "                            input_length=maxlen))\n",
    "model.add(layers.Conv1D(512, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and run the model of method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_acc:.4f}.hdf5', monitor = 'val_acc', verbose = 1, save_best_only=True, mode='auto')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_full, y_train,\n",
    "                    epochs=10, callbacks = [checkpoint], verbose =2,\n",
    "                    validation_data=(X_test_full, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and submission for model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = np.load(\"Datasets/twitter-datasets/reduced_tweets_test.npy\")\n",
    "tweets_test = tokenizer.texts_to_sequences(tweets_test)\n",
    "tweets_test = pad_sequences(tweets_test, padding='post', maxlen=maxlen)\n",
    "y = model.predict_classes(tweets_test)\n",
    "\n",
    "# make csv\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "    id = 1\n",
    "    for i in y:\n",
    "        if i == 0:\n",
    "            i = -1\n",
    "        if i == 1:\n",
    "            i = 1\n",
    "        l = str(id) + \",\" + str(i) + \"\\n\"\n",
    "        f.write(l)\n",
    "        id = id + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
